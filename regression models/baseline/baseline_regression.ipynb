{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, SplineTransformer\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "DATA_PATH = r\"contracts_with_isi_v2_SWEEP_WIDE_WITH_KEYS_PLUS_CPI.csv\"\n",
    "\n",
    "BAT_RATES_PATH = r\"batting_rates_by_season.csv\"\n",
    "PIT_RATES_PATH = r\"pitching_rates_by_season.csv\"\n",
    "\n",
    "# NEW\n",
    "DEF_STATS_PATH = r\"defensive_stats.csv\"\n",
    "STATCAST_PIT_PATH = r\"statcast_pitching_2015_2025.csv\"\n",
    "\n",
    "# Target\n",
    "REG_TARGET = \"guarantee_real_per_year_2025\"\n",
    "\n",
    "# Time split\n",
    "TRAIN_YEARS = {2020, 2021, 2022, 2023}\n",
    "TEST_YEARS  = {2024, 2025}\n",
    "\n",
    "# Window settings for panels\n",
    "PRE_YEARS = 3\n",
    "\n",
    "# Exports\n",
    "OUT_REG_RESULTS = r\"model_comparison_regression_BASELINE_new.csv\"\n",
    "\n",
    "\n",
    "# Base features\n",
    "BASE_NUMERIC = [\n",
    "    \"age_at_signing\",\n",
    "    \"years_int\",\n",
    "    \"opt_out_flag\",\n",
    "    \"year\",\n",
    "    \"is_pitcher_flag\",\n",
    "]\n",
    "\n",
    "BASE_CATEGORICAL = [\n",
    "    \"position\",\n",
    "    \"qualifying_offer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73156ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# drop dups, preserves order\n",
    "def unique_list(seq):\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "# drop dup col names\n",
    "def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "PITCHER_PREFIXES = (\"P\", \"SP\", \"RP\", \"RHP\", \"LHP\")\n",
    "\n",
    "def is_pitcher(pos) -> int:\n",
    "    if pd.isna(pos):\n",
    "        return 0\n",
    "    s = str(pos).strip().upper()\n",
    "    # handles pitcher positional variations\n",
    "    return int(s.startswith(PITCHER_PREFIXES) or (\"RHP\" in s) or (\"LHP\" in s))\n",
    "\n",
    "def time_split(df: pd.DataFrame, year_col: str = \"year\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    y = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    train = df[y.isin(TRAIN_YEARS)].copy()\n",
    "    test  = df[y.isin(TEST_YEARS)].copy()\n",
    "    return train, test\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    # Data integrity check\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"No y_true/y_pred pairs available after filtering.\")\n",
    "\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"n_eval\": int(mask.sum())}\n",
    "\n",
    "def _safe_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _weighted_mean(series: pd.Series, weights: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\").fillna(0.0)\n",
    "    mask = np.isfinite(s) & np.isfinite(w) & (w > 0)\n",
    "    if mask.sum() == 0:\n",
    "        s2 = s[np.isfinite(s)]\n",
    "        return float(s2.mean()) if len(s2) else np.nan\n",
    "    return float(np.average(s[mask], weights=w[mask]))\n",
    "\n",
    "def add_pre_rate_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    season_rates: pd.DataFrame,\n",
    "    *,\n",
    "    rate_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-window aggregate computations for seasons in window\n",
    "    \"\"\"\n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"key_fangraphs\"] = pd.to_numeric(dfc[\"key_fangraphs\"], errors=\"coerce\")\n",
    "    dfc[\"year\"] = pd.to_numeric(dfc[\"year\"], errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfs = season_rates.copy()\n",
    "    dfs[\"playerId\"] = pd.to_numeric(dfs[\"playerId\"], errors=\"coerce\")\n",
    "    dfs[\"Season\"] = pd.to_numeric(dfs[\"Season\"], errors=\"coerce\")\n",
    "\n",
    "    dfs = _safe_numeric(dfs, rate_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"key_fangraphs\", \"year\"]].merge(\n",
    "        dfs,\n",
    "        left_on=\"key_fangraphs\",\n",
    "        right_on=\"playerId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    m[\"lb_start\"] = m[\"year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"year\"] - 1\n",
    "    m = m[m[\"Season\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"Season\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        rel_sum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_reliability_sum\")\n",
    "        out = out.merge(rel_sum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_reliability_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_reliability_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for rc in rate_cols:\n",
    "        feat_name = f\"{prefix}_pre_{rc}\"\n",
    "        if rc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[rc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[rc].mean().rename(feat_name)\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre panel features\n",
    "\n",
    "def add_pre_panel_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str,\n",
    "    panel_key_col: str,\n",
    "    contract_year_col: str,\n",
    "    panel_year_col: str,\n",
    "    feature_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Generic season panel aggregator (only used for defense and statcast files) \"\"\"\n",
    "    # Data integrity\n",
    "    # Missing value check\n",
    "    missing_contract = [c for c in [contract_key_col, contract_year_col] if c not in contracts.columns]\n",
    "    missing_panel = [c for c in [panel_key_col, panel_year_col] if c not in panel.columns]\n",
    "    if missing_contract:\n",
    "        raise KeyError(f\"[add_pre_panel_features] contracts missing: {missing_contract}\")\n",
    "    if missing_panel:\n",
    "        raise KeyError(f\"[add_pre_panel_features] panel missing: {missing_panel}\")\n",
    "\n",
    "    out = contracts.copy()\n",
    "    out[\"_row_id\"] = np.arange(len(out), dtype=int)\n",
    "\n",
    "    # temp cols\n",
    "    dfc = out[[\"_row_id\", contract_key_col, contract_year_col]].copy()\n",
    "    dfc[\"_contract_key\"] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[\"_contract_year\"] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "\n",
    "    # returns necessary cols only\n",
    "    keep_feats = [c for c in feature_cols if c in panel.columns]\n",
    "    keep_cols = [panel_key_col, panel_year_col] + keep_feats\n",
    "    if weight_col and weight_col in panel.columns and weight_col not in keep_cols:\n",
    "        keep_cols.append(weight_col)\n",
    "\n",
    "    p = panel[keep_cols].copy()\n",
    "    p[\"_panel_key\"] = pd.to_numeric(p[panel_key_col], errors=\"coerce\")\n",
    "    p[\"_panel_year\"] = pd.to_numeric(p[panel_year_col], errors=\"coerce\")\n",
    "\n",
    "    for c in keep_feats:\n",
    "        p[c] = pd.to_numeric(p[c], errors=\"coerce\")\n",
    "    if weight_col and weight_col in p.columns:\n",
    "        p[weight_col] = pd.to_numeric(p[weight_col], errors=\"coerce\")\n",
    "\n",
    "    merge_cols = [\"_panel_key\", \"_panel_year\"] + keep_feats + ([weight_col] if (weight_col and weight_col in p.columns) else [])\n",
    "    m = dfc[[\"_row_id\", \"_contract_key\", \"_contract_year\"]].merge(\n",
    "        p[merge_cols],\n",
    "        left_on=\"_contract_key\",\n",
    "        right_on=\"_panel_key\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[\"_contract_year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"_contract_year\"] - 1\n",
    "    m = m[m[\"_panel_year\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"_panel_year\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, on=\"_row_id\", how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    # Weighted sums\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        wsum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_weight_sum\")\n",
    "        out = out.merge(wsum, on=\"_row_id\", how=\"left\")\n",
    "        out[f\"{prefix}_pre_weight_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_weight_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Aggregate features where necessary\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        w = pd.to_numeric(m[weight_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            x = pd.to_numeric(m[fc], errors=\"coerce\")\n",
    "            num = (x * w).groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            den = w.groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            wmean = (num / den).replace([np.inf, -np.inf], np.nan).rename(feat_name)\n",
    "            out = out.merge(wmean, on=\"_row_id\", how=\"left\")\n",
    "    else:\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            mean = m.groupby(\"_row_id\")[fc].mean().rename(feat_name)\n",
    "            out = out.merge(mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def make_preprocessor(\n",
    "    numeric_features: List[str],\n",
    "    categorical_features: List[str],\n",
    "    use_splines: bool\n",
    ") -> ColumnTransformer:\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "\n",
    "    if use_splines:\n",
    "        num_steps += [\n",
    "            (\"splines\", SplineTransformer(n_knots=6, degree=3, include_bias=False)),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        ]\n",
    "    else:\n",
    "        num_steps += [(\"scaler\", StandardScaler())]\n",
    "\n",
    "    num_pipe = Pipeline(steps=num_steps)\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_features),\n",
    "            (\"cat\", cat_pipe, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7975a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "# Using LR, EN, RFG, GAMSL, GAMSEN, XGB Sweep\n",
    "def get_regression_models(random_state: int = 42) -> Dict[str, object]:\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.002, l1_ratio=0.35, random_state=random_state, max_iter=20000),\n",
    "        \"RandomForest\": RandomForestRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        \"GAM_Splines_Linear\": LinearRegression(),\n",
    "        \"GAM_Splines_ElasticNet\": ElasticNet(alpha=0.0015, l1_ratio=0.2, random_state=random_state, max_iter=20000),\n",
    "    }\n",
    "\n",
    "    # XGB sweep config\n",
    "    xgb_grid = [\n",
    "        {\"n_estimators\": 600,  \"learning_rate\": 0.05, \"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 900,  \"learning_rate\": 0.04, \"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 1200, \"learning_rate\": 0.03, \"max_depth\": 4, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 1400, \"learning_rate\": 0.025,\"max_depth\": 4, \"subsample\": 0.80, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.5},\n",
    "        {\"n_estimators\": 1600, \"learning_rate\": 0.02, \"max_depth\": 5, \"subsample\": 0.80, \"colsample_bytree\": 0.80, \"reg_lambda\": 2.0},\n",
    "        {\"n_estimators\": 1000, \"learning_rate\": 0.001,\"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 1000, \"learning_rate\": 0.005,\"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 2500, \"learning_rate\": 0.001,\"max_depth\": 4, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 2500, \"learning_rate\": 0.005,\"max_depth\": 4, \"subsample\": 0.80, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.5},\n",
    "        {\"n_estimators\": 5000, \"learning_rate\": 0.001,\"max_depth\": 5, \"subsample\": 0.80, \"colsample_bytree\": 0.80, \"reg_lambda\": 2.0},\n",
    "        {\"n_estimators\": 5000, \"learning_rate\": 0.005,\"max_depth\": 5, \"subsample\": 0.80, \"colsample_bytree\": 0.80, \"reg_lambda\": 2.0},\n",
    "    ]\n",
    "\n",
    "    # XGB Setup (applied once, rather than for each sweep iteration)\n",
    "    xgb_defaults = dict(\n",
    "        random_state=random_state,\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_jobs=-1,\n",
    "        reg_alpha=0.0,\n",
    "        min_child_weight=5,\n",
    "    )\n",
    "\n",
    "    # assigns a name for each XGB sweep model\n",
    "    # ensures no overwriting of previous iterations\n",
    "    for i, params in enumerate(xgb_grid, start=1):\n",
    "        name = f\"XGB_{i:02d}_ne{params['n_estimators']}_lr{params['learning_rate']}_md{params['max_depth']}\"\n",
    "        models[name] = XGBRegressor(**xgb_defaults, **params)\n",
    "\n",
    "    return models\n",
    "\n",
    "# Model training\n",
    "def fit_predict_regression(\n",
    "    model_name: str, model, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame,\n",
    "    *, numeric_features: List[str], categorical_features: List[str],\n",
    "    ) -> Tuple[np.ndarray, object]:\n",
    "\n",
    "    use_splines = model_name.startswith(\"GAM_Splines_\")\n",
    "    pre = make_preprocessor(numeric_features, categorical_features, use_splines=use_splines)\n",
    "\n",
    "    reg = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "    ttr = TransformedTargetRegressor(regressor=reg, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "    ttr.fit(X_train, y_train)\n",
    "    preds = ttr.predict(X_test)\n",
    "    return preds, ttr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runs baseline regression models that are set up above\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # data loader\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    bat_rates = pd.read_csv(BAT_RATES_PATH)\n",
    "    pit_rates = pd.read_csv(PIT_RATES_PATH)\n",
    "    def_stats = pd.read_csv(DEF_STATS_PATH)\n",
    "    sc_pit    = pd.read_csv(STATCAST_PIT_PATH)\n",
    "\n",
    "    # Data integrity\n",
    "    df = dedupe_columns(df)\n",
    "\n",
    "    df[\"term_start_year\"] = pd.to_numeric(df.get(\"term_start_year\"), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"term_start_year\"]).copy()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"term_start_year\"], errors=\"coerce\")\n",
    "\n",
    "    # Restrict contract years are 2020-2025\n",
    "    df = df[(df[\"year\"] >= 2020) & (df[\"year\"] <= 2025)].copy()\n",
    "\n",
    "    # Restricts contract terms to 1-5 years\n",
    "    MAX_YEARS = 5\n",
    "    df[\"years_int\"] = pd.to_numeric(df[\"years_int\"], errors=\"coerce\")\n",
    "\n",
    "    before = len(df)\n",
    "    df = df[df[\"years_int\"].notna() & (df[\"years_int\"] <= MAX_YEARS)].copy()\n",
    "    after = len(df)\n",
    "\n",
    "    print(f\"[FILTER] Dropped {before - after} contracts with years_int > {MAX_YEARS} (or missing)\")\n",
    "\n",
    "    # coerce keys\n",
    "    for c in [\"key_fangraphs\", \"key_mlbam\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Assign pitcher flag\n",
    "    if \"position\" in df.columns:\n",
    "        df[\"is_pitcher_flag\"] = df[\"position\"].map(is_pitcher).astype(int)\n",
    "    else:\n",
    "        df[\"is_pitcher_flag\"] = 0\n",
    "\n",
    "    # Data integrity\n",
    "    # Confirms necessary cols are present\n",
    "    print(\"\\n--- DEBUG: before panel merges ---\")\n",
    "    print(\"df has year?\", \"year\" in df.columns)\n",
    "    print(\"df year dtype:\", df[\"year\"].dtype)\n",
    "    print(\"df key_mlbam dtype:\", df[\"key_mlbam\"].dtype if \"key_mlbam\" in df.columns else None)\n",
    "    print(\"df key_fangraphs dtype:\", df[\"key_fangraphs\"].dtype if \"key_fangraphs\" in df.columns else None)\n",
    "    print(\"sc_pit has year?\", \"year\" in sc_pit.columns)\n",
    "    print(\"def_stats has year?\", \"year\" in def_stats.columns)\n",
    "\n",
    "\n",
    "    BAT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"PA\"}\n",
    "    PIT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"IP\", \"TBF\"}\n",
    "\n",
    "    bat_rate_cols = [\n",
    "        c for c in bat_rates.columns\n",
    "        if c not in BAT_EXCLUDE\n",
    "        and c != \"bat_rate_reliability\"\n",
    "        and not c.endswith(\"_dup\")\n",
    "    ]\n",
    "    pit_rate_cols = [\n",
    "        c for c in pit_rates.columns\n",
    "        if c not in PIT_EXCLUDE\n",
    "        and c != \"pit_rate_reliability\"\n",
    "        and not c.endswith(\"_dup\")\n",
    "    ]\n",
    "\n",
    "    # Data integrity\n",
    "    bat_weight_col = \"bat_rate_reliability\" if \"bat_rate_reliability\" in bat_rates.columns else None\n",
    "    pit_weight_col = \"pit_rate_reliability\" if \"pit_rate_reliability\" in pit_rates.columns else None\n",
    "\n",
    "    df = add_pre_rate_features(\n",
    "        df,\n",
    "        bat_rates,\n",
    "        rate_cols=bat_rate_cols,\n",
    "        weight_col=bat_weight_col,\n",
    "        prefix=\"bat\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "\n",
    "    df = add_pre_rate_features(\n",
    "        df,\n",
    "        pit_rates,\n",
    "        rate_cols=pit_rate_cols,\n",
    "        weight_col=pit_weight_col,\n",
    "        prefix=\"pit\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Add defensive features\n",
    "\n",
    "    def_feature_cols = [c for c in [\"defensive_runs_saved\", \"fielding_percentage\", \"Errors\"] if c in def_stats.columns]\n",
    "\n",
    "    df = add_pre_panel_features(\n",
    "        contracts=df,\n",
    "        panel=def_stats,\n",
    "        contract_key_col=\"key_mlbam\",\n",
    "        panel_key_col=\"MLBAMID\",\n",
    "        contract_year_col=\"year\",\n",
    "        panel_year_col=\"year\",\n",
    "        feature_cols=def_feature_cols,\n",
    "        weight_col=\"Innings_played\" if \"Innings_played\" in def_stats.columns else None,\n",
    "        prefix=\"def\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "\n",
    "    # Add statcast pitching features\n",
    "    sc_feature_cols = [\n",
    "        c for c in [\n",
    "            \"fastball_avg_speed\",\n",
    "            \"whiff_percent\",\n",
    "            \"hard_hit_percent\",\n",
    "            \"barrel_batted_rate\",\n",
    "            \"exit_velocity_avg\",\n",
    "            \"swing_percent\",\n",
    "        ]\n",
    "        if c in sc_pit.columns\n",
    "    ]\n",
    "\n",
    "    df = add_pre_panel_features(\n",
    "        contracts=df,\n",
    "        panel=sc_pit,\n",
    "        contract_key_col=\"key_mlbam\",\n",
    "        panel_key_col=\"player_id\",\n",
    "        contract_year_col=\"year\",\n",
    "        panel_year_col=\"year\",\n",
    "        feature_cols=sc_feature_cols,\n",
    "        weight_col=\"pa\" if \"pa\" in sc_pit.columns else None,\n",
    "        prefix=\"scpit\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "\n",
    "    df = dedupe_columns(df)\n",
    "\n",
    "    # Data Integrity\n",
    "    for col in [\"has_bat_pre\", \"has_pit_pre\", \"has_def_pre\", \"has_scpit_pre\"]:\n",
    "        if col in df.columns:\n",
    "            print(f\"{col}: {df[col].mean():.3f} coverage ({int(df[col].sum())}/{len(df)})\")\n",
    "\n",
    "    # Audit\n",
    "    DUP_KEY = [\n",
    "        \"player_name\",\n",
    "        \"term_start_year\",\n",
    "        \"new_club\",\n",
    "        \"years_int\",\n",
    "        REG_TARGET,\n",
    "    ]\n",
    "    DUP_KEY = [c for c in DUP_KEY if c in df.columns]\n",
    "    dups = df[df.duplicated(DUP_KEY, keep=False)].sort_values(DUP_KEY) if len(DUP_KEY) else pd.DataFrame()\n",
    "\n",
    "    if len(dups):\n",
    "        dups.to_csv(r\"dup_contract_audit_BASELINE.csv\", index=False)\n",
    "        print(f\"Saved dup audit: {len(dups)} rows\")\n",
    "\n",
    "    if len(DUP_KEY):\n",
    "        df = df.drop_duplicates(DUP_KEY, keep=\"first\").copy()\n",
    "\n",
    "    print(\"\\nDuplicate audit:\")\n",
    "    print(\"Key columns used:\", DUP_KEY)\n",
    "    print(\"Total rows:\", len(df))\n",
    "    print(\"Duplicate rows (including both originals and copies):\", len(dups))\n",
    "\n",
    "\n",
    "    # Runs Regresion Model\n",
    "    df_reg = df.dropna(subset=[REG_TARGET]).copy()\n",
    "    df_reg = dedupe_columns(df_reg)\n",
    "\n",
    "    # Final Integrity check\n",
    "    # ensures target row is cleaned and usable\n",
    "    df_reg[REG_TARGET] = (\n",
    "        df_reg[REG_TARGET]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    df_reg[REG_TARGET] = pd.to_numeric(df_reg[REG_TARGET], errors=\"coerce\")\n",
    "    df_reg = df_reg[np.isfinite(df_reg[REG_TARGET])].copy()\n",
    "    df_reg = df_reg[df_reg[REG_TARGET] >= 0].copy()\n",
    "\n",
    "    train_reg_full, test_reg_full = time_split(df_reg, year_col=\"year\")\n",
    "\n",
    "    cov_suffixes = (\"_pre_seasons\", \"_pre_reliability_sum\", \"_pre_weight_sum\")\n",
    "    generated_cov_feats = [\n",
    "        c for c in df_reg.columns\n",
    "        if c in {\"has_bat_pre\", \"has_pit_pre\", \"has_def_pre\", \"has_scpit_pre\"}\n",
    "        or c.endswith(cov_suffixes)\n",
    "    ]\n",
    "\n",
    "    PREFIXES = (\"bat_pre_\", \"pit_pre_\", \"def_pre_\", \"scpit_pre_\")\n",
    "    generated_rate_feats = [\n",
    "        c for c in df_reg.columns\n",
    "        if c.startswith(PREFIXES) and c not in generated_cov_feats\n",
    "    ]\n",
    "\n",
    "    numeric_features = unique_list([\n",
    "        c for c in (BASE_NUMERIC + generated_rate_feats + generated_cov_feats)\n",
    "        if c in df_reg.columns\n",
    "    ])\n",
    "    categorical_features = unique_list([\n",
    "        c for c in (BASE_CATEGORICAL)\n",
    "        if c in df_reg.columns\n",
    "    ])\n",
    "\n",
    "    # standard with no percentile cutoff\n",
    "    def run_variant(train_reg: pd.DataFrame, test_reg: pd.DataFrame, *, variant: str, cut_pct: float | None, cut_value: float | None):\n",
    "        X_train = train_reg[numeric_features + categorical_features]\n",
    "        y_train = pd.to_numeric(train_reg[REG_TARGET], errors=\"coerce\")\n",
    "        X_test  = test_reg[numeric_features + categorical_features]\n",
    "        y_test  = pd.to_numeric(test_reg[REG_TARGET], errors=\"coerce\")\n",
    "\n",
    "        rows = []\n",
    "        for model_name, model in get_regression_models().items():\n",
    "            preds, _ = fit_predict_regression(\n",
    "                model_name, model, X_train, y_train, X_test,\n",
    "                numeric_features=numeric_features,\n",
    "                categorical_features=categorical_features\n",
    "            )\n",
    "            mets = regression_metrics(y_test.values, preds)\n",
    "\n",
    "            rows.append({\n",
    "                \"run\": \"BASELINE_ONLY\",\n",
    "                \"variant\": variant,                 \n",
    "                \"cut_pct\": cut_pct,             \n",
    "                \"cut_value\": cut_value,\n",
    "                \"model\": model_name,\n",
    "                \"n_train\": len(train_reg),\n",
    "                \"n_test\": len(test_reg),\n",
    "                \"n_features_numeric\": len(numeric_features),\n",
    "                \"n_features_categorical\": len(categorical_features),\n",
    "                **mets,\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    reg_results_rows = []\n",
    "\n",
    "    # Unfiltered variant\n",
    "    reg_results_rows += run_variant(\n",
    "        train_reg_full, test_reg_full,\n",
    "        variant=\"UNFILTERED\",\n",
    "        cut_pct=None,\n",
    "        cut_value=None,\n",
    "    )\n",
    "\n",
    "    # Variant 5% Cutoff\n",
    "    REMOVE_TOP_PCTL = 0.95\n",
    "    aav_cutoff = train_reg_full[REG_TARGET].quantile(REMOVE_TOP_PCTL)\n",
    "\n",
    "    train_reg_cut = train_reg_full[train_reg_full[REG_TARGET] <= aav_cutoff].copy()\n",
    "    test_reg_cut  = test_reg_full[test_reg_full[REG_TARGET] <= aav_cutoff].copy()\n",
    "\n",
    "    print(f\"[TOP5% FILTER] Training Contract Cutoff ({REMOVE_TOP_PCTL:.0%}): {aav_cutoff:,.0f}\")\n",
    "    print(f\"[TOP5% FILTER] n_train={len(train_reg_cut)} | n_test={len(test_reg_cut)}\")\n",
    "\n",
    "    reg_results_rows += run_variant(\n",
    "        train_reg_cut, test_reg_cut,\n",
    "        variant=\"TOP5_REMOVED\",\n",
    "        cut_pct=REMOVE_TOP_PCTL,\n",
    "        cut_value=float(aav_cutoff),\n",
    "    )\n",
    "\n",
    "    reg_results = pd.DataFrame(reg_results_rows).sort_values([\"variant\", \"MAE\"])\n",
    "    reg_results.to_csv(OUT_REG_RESULTS, index=False)\n",
    "\n",
    "    print(\"\\nSaved regression comparison:\", OUT_REG_RESULTS)\n",
    "\n",
    "    # prints top 5 models for each variant\n",
    "    for v in reg_results[\"variant\"].unique():\n",
    "        print(f\"\\nTop results for {v} (sorted by MAE):\")\n",
    "        sub = reg_results[reg_results[\"variant\"] == v].sort_values(\"MAE\").head(10)\n",
    "        print(sub[[\"model\", \"MAE\", \"RMSE\", \"R2\", \"n_train\", \"n_test\", \"cut_value\"]].round(4).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabr_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
