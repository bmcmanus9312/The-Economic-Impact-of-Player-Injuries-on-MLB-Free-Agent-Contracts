{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLB Contract Regression v2 - Strategic Feature Engineering\n",
    "\n",
    "Intended to incorporate new feature engineering\n",
    "\n",
    "New features include:\n",
    "    ISI * Age/Performance interactions\n",
    "    Pitcher-specific injury features\n",
    "    More detailed injury type breakdowon\n",
    "    Injury temporal trends, analyzing positive or negative injury history trends\n",
    "    Position-specific encoding\n",
    "\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations: 614,400\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "DATA_PATH = r\"contracts_with_isi_v2_SWEEP_WIDE_WITH_KEYS_PLUS_CPI.csv\"\n",
    "BAT_RATES_PATH = r\"batting_rates_by_season.csv\"\n",
    "PIT_RATES_PATH = r\"pitching_rates_by_season.csv\"\n",
    "DEF_STATS_PATH = r\"defensive_stats.csv\"\n",
    "STATCAST_PIT_PATH = r\"statcast_pitching_2015_2025.csv\"\n",
    "\n",
    "\n",
    "# Output path for results and model weights\n",
    "OUT_RESULTS = r\"regression_v2_results.csv\"\n",
    "\n",
    "# establishes time limit for how long the model can run\n",
    "TIME_LIMIT_HOURS = 2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Time split\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]\n",
    "TEST_YEARS = [2024, 2025]\n",
    "\n",
    "# Contract filters\n",
    "# utilzied in config rather than further down, as in original baseline model\n",
    "# max years filtered max contract term\n",
    "# top pctl filters top_n percent of contract AAV\n",
    "MAX_YEARS = 5\n",
    "REMOVE_TOP_PCTL = 0.95\n",
    "\n",
    "# Target\n",
    "REG_TARGET = \"guarantee_real_per_year_2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b868be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid Search\n",
    "# total of 614,400 potential combinations\n",
    "\n",
    "PARAM_GRID = {\n",
    "    'n_estimators': [500, 1000, 1500, 2000, 2500, 3000],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.03, 0.05],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [3, 5, 7, 10],\n",
    "    'subsample': [0.7, 0.8, 0.85, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.85, 0.9],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0, 3.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3, 0.5],\n",
    "}\n",
    "\n",
    "# Prints total number of combos\n",
    "total_combos = np.prod(\n",
    "    [len(v) for v in PARAM_GRID.values()]\n",
    ")\n",
    "\n",
    "print(f\"Total possible combos: {total_combos:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb234180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# drop dups, preserves order\n",
    "def unique_list(seq):\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "# drop dup col names\n",
    "def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "def is_pitcher(pos) -> int:\n",
    "    PITCHER_PREFIXES = (\"P\", \"SP\", \"RP\", \"RHP\", \"LHP\")\n",
    "    if pd.isna(pos):\n",
    "        return 0\n",
    "    s = str(pos).strip().upper()\n",
    "    # handles pitcher positional variations\n",
    "    return int(s.startswith(PITCHER_PREFIXES) or (\"RHP\" in s) or (\"LHP\" in s))\n",
    "\n",
    "def time_split(df: pd.DataFrame, year_col: str = \"year\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    y = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    train = df[y.isin(TRAIN_YEARS)].copy()\n",
    "    test = df[y.isin(TEST_YEARS)].copy()\n",
    "    return train, test\n",
    "\n",
    "# Data integrity check\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"No finite y_true/y_pred pairs available.\")\n",
    "    \n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    \n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "def safe_cols(df: pd.DataFrame, col_list: List[str]) -> List[str]:\n",
    "    return [c for c in col_list if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# replcaited from v1\n",
    "\n",
    "def _safe_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _weighted_mean(series: pd.Series, weights: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\").fillna(0.0)\n",
    "    mask = np.isfinite(s) & np.isfinite(w) & (w > 0)\n",
    "    if mask.sum() == 0:\n",
    "        s2 = s[np.isfinite(s)]\n",
    "        return float(s2.mean()) if len(s2) else np.nan\n",
    "    return float(np.average(s[mask], weights=w[mask]))\n",
    "\n",
    "def add_pre_rate_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    season_rates: pd.DataFrame,\n",
    "    *,\n",
    "    rate_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-window aggregate computations for seasons in window\n",
    "    \"\"\"    \n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"key_fangraphs\"] = pd.to_numeric(dfc[\"key_fangraphs\"], errors=\"coerce\")\n",
    "    dfc[\"year\"] = pd.to_numeric(dfc[\"year\"], errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfs = season_rates.copy()\n",
    "    dfs[\"playerId\"] = pd.to_numeric(dfs[\"playerId\"], errors=\"coerce\")\n",
    "    dfs[\"Season\"] = pd.to_numeric(dfs[\"Season\"], errors=\"coerce\")\n",
    "    dfs = _safe_numeric(dfs, rate_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"key_fangraphs\", \"year\"]].merge(\n",
    "        dfs, left_on=\"key_fangraphs\", right_on=\"playerId\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    m[\"lb_start\"] = m[\"year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"year\"] - 1\n",
    "    m = m[m[\"Season\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"Season\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        rel_sum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_reliability_sum\")\n",
    "        out = out.merge(rel_sum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_reliability_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_reliability_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for rc in rate_cols:\n",
    "        feat_name = f\"{prefix}_pre_{rc}\"\n",
    "        if rc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[rc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[rc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre panel features\n",
    "\n",
    "def add_pre_panel_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str,\n",
    "    panel_key_col: str,\n",
    "    contract_year_col: str,\n",
    "    panel_year_col: str,\n",
    "    feature_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Generic season panel aggregator (only used for defense and statcast files) \"\"\"\n",
    "    # reduced implementation of data integrity checks since data\n",
    "    # is known to be clean due to success of the baseline model\n",
    "\n",
    "    # temp cols\n",
    "    out = contracts.copy()\n",
    "    out[\"_row_id\"] = np.arange(len(out), dtype=int)\n",
    "\n",
    "    dfc = out[[\"_row_id\", contract_key_col, contract_year_col]].copy()\n",
    "    dfc[\"_contract_key\"] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[\"_contract_year\"] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "\n",
    "    keep_feats = [c for c in feature_cols if c in panel.columns]\n",
    "    keep_cols = [panel_key_col, panel_year_col] + keep_feats\n",
    "    if weight_col and weight_col in panel.columns and weight_col not in keep_cols:\n",
    "        keep_cols.append(weight_col)\n",
    "\n",
    "    p = panel[keep_cols].copy()\n",
    "    p[\"_panel_key\"] = pd.to_numeric(p[panel_key_col], errors=\"coerce\")\n",
    "    p[\"_panel_year\"] = pd.to_numeric(p[panel_year_col], errors=\"coerce\")\n",
    "\n",
    "    for c in keep_feats:\n",
    "        p[c] = pd.to_numeric(p[c], errors=\"coerce\")\n",
    "    if weight_col and weight_col in p.columns:\n",
    "        p[weight_col] = pd.to_numeric(p[weight_col], errors=\"coerce\")\n",
    "\n",
    "    merge_cols = [\"_panel_key\", \"_panel_year\"] + keep_feats + ([weight_col] if (weight_col and weight_col in p.columns) else [])\n",
    "    m = dfc[[\"_row_id\", \"_contract_key\", \"_contract_year\"]].merge(\n",
    "        p[merge_cols], left_on=\"_contract_key\", right_on=\"_panel_key\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[\"_contract_year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"_contract_year\"] - 1\n",
    "    m = m[m[\"_panel_year\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"_panel_year\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, on=\"_row_id\", how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    # Weighted sums\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        wsum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_weight_sum\")\n",
    "        out = out.merge(wsum, on=\"_row_id\", how=\"left\")\n",
    "        out[f\"{prefix}_pre_weight_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_weight_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Aggregate features where necessary\n",
    "    for fc in keep_feats:\n",
    "        feat_name = f\"{prefix}_pre_{fc}\"\n",
    "        if fc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[fc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[fc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for v2\n",
    "\n",
    "def engineer_v2_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    v2 feature engineering\n",
    "    Emphasis on feature interactions and injury-specific integrations\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # tells model to use looback=5 and lambda=35\n",
    "    isi_col = 'ISI_lb5_lamdba_35'\n",
    "    \n",
    "    # Data Integrity\n",
    "    if isi_col not in df.columns:\n",
    "        print(f\"WARNING: {isi_col} not found.\")\n",
    "        return df\n",
    "    \n",
    "    feature_count_start = len(df.columns)\n",
    "\n",
    "##############\n",
    "# New Features\n",
    "##############\n",
    "\n",
    "    # Age interactions\n",
    "    # attempting to show that older players + injuries = larger injury discount\n",
    "    # age at signing col NECESSARY\n",
    "    if 'age_at_signing' in df.columns:\n",
    "        df['isi_x_age'] = df[isi_col] * df['age_at_signing']\n",
    "        # applies a squared function to age\n",
    "        # exponential age factor\n",
    "        df['isi_x_age_squared'] = df[isi_col] * (df['age_at_signing'] ** 2)\n",
    "        # 'old' players defined as 32 years old or higher\n",
    "        df['isi_x_old'] = df[isi_col] * (df['age_at_signing'] >= 32).astype(int)\n",
    "    \n",
    "    # contract term length interactions\n",
    "    if 'years_int' in df.columns:\n",
    "        df['isi_x_years'] = df[isi_col] * df['years_int']\n",
    "        # for analysis, long term is 4-5 years (6+ term years are excluded)\n",
    "        df['isi_x_longterm'] = df[isi_col] * (df['years_int'] >= 4).astype(int)\n",
    "    \n",
    "    # advanced batting metric interactions\n",
    "    for stat in safe_cols(df, ['bat_pre_wRC+', 'bat_pre_OPS', 'bat_pre_wOBA']):\n",
    "        df[f'isi_x_{stat}'] = df[isi_col] * df[stat]\n",
    "    \n",
    "    # advanced pitching metric interactions\n",
    "    for stat in safe_cols(df, ['pit_pre_FIP', 'pit_pre_strikeout_percent']):\n",
    "        df[f'isi_x_{stat}'] = df[isi_col] * df[stat]\n",
    "    \n",
    "    # prints the total number of interaction features that were created\n",
    "    print(f\"Total interaction features: {len([c for c in df.columns if 'isi_x_' in c])}\")\n",
    "    \n",
    "\n",
    "    # injury type decomposition\n",
    "    # utilizes two lookback varations, one lambda\n",
    "    for isi_variant in ['ISI_lb3_lamdba_35', 'ISI_lb5_lamdba_35']:\n",
    "        if isi_variant not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        suffix = isi_variant.replace('ISI', '')\n",
    "        \n",
    "        # Surgery factor\n",
    "        surgery_col = f'any_surgery_flag{suffix}'\n",
    "        if surgery_col in df.columns:\n",
    "            df[f'surgery_burden{suffix}'] = df[isi_variant] * df[surgery_col]\n",
    "            df[f'no_surgery_but_isi{suffix}'] = df[isi_variant] * (1 - df[surgery_col])\n",
    "        \n",
    "        # Structural damage factor\n",
    "        structural_col = f'any_structural_flag{suffix}'\n",
    "        if structural_col in df.columns:\n",
    "            df[f'structural_burden{suffix}'] = df[isi_variant] * df[structural_col]\n",
    "        \n",
    "        # Arm injury factor\n",
    "        # pitcher identiciation only\n",
    "        arm_share_col = f'arm_share_days{suffix}'\n",
    "        if arm_share_col in df.columns and 'is_pitcher_flag' in df.columns:\n",
    "            df[f'pitcher_arm_burden{suffix}'] = df[arm_share_col] * df['is_pitcher_flag']\n",
    "            df[f'pitcher_isi_x_arm{suffix}'] = df[isi_variant] * df[arm_share_col] * df['is_pitcher_flag']\n",
    "    \n",
    "    # prints the total number of interaction features that were created\n",
    "    print(f\"Created {len([c for c in df.columns if 'burden' in c])} injury type features\")\n",
    "\n",
    "    # positional factor\n",
    "    df['isi_pitcher'] = df[isi_col] * df['is_pitcher_flag']\n",
    "    df['isi_position_player'] = df[isi_col] * (1 - df['is_pitcher_flag'])\n",
    "    \n",
    "    # defines starter and reliver positions where available\n",
    "    if 'position' in df.columns:\n",
    "        df['isi_starter'] = df[isi_col] * (df['position'] == 'SP').astype(int)\n",
    "        df['isi_reliever'] = df[isi_col] * (df['position'] == 'RP').astype(int)\n",
    "        \n",
    "        # premium positions defined as positions where scarcity exists\n",
    "        # ADJUST AS NEEDED\n",
    "        premium_pos = ['SS', 'C', 'CF']\n",
    "        df['isi_premium_pos'] = df[isi_col] * df['position'].isin(premium_pos).astype(int)\n",
    "    \n",
    "    # prints the total number of interaction features that were created\n",
    "    print(f\"Created {len([c for c in df.columns if c.startswith('isi_') and any(x in c for x in ['pitcher', 'position', 'starter', 'reliever', 'premium'])])} position features\")\n",
    "    \n",
    "    \n",
    "    # temporal features\n",
    "    # utilizes both a three and five year lookback period\n",
    "    isi_recent = 'ISI_lb3_lamdba_35'\n",
    "    isi_full = 'ISI_lb5_lamdba_35'\n",
    "    \n",
    "    if isi_recent in df.columns and isi_full in df.columns:\n",
    "        # ISI acceleration feature intended to diagnose health changes\n",
    "        # improving is good, worsening signals a durability decrease\n",
    "        df['isi_acceleration'] = df[isi_recent] - df[isi_full]\n",
    "        df['isi_worsening'] = (df['isi_acceleration'] > 0).astype(int)\n",
    "        df['isi_improving'] = (df['isi_acceleration'] < -0.05).astype(int)\n",
    "        \n",
    "        df['isi_trend_ratio'] = df[isi_recent] / (df[isi_full] + 0.01)\n",
    "            \n",
    "    # Fast vs slow decay (recency signal)\n",
    "    # utilizes a fast and slow recency weight\n",
    "    # lookback=5 lambda=0.70 (increased emphasis on more recent injuries)\n",
    "    # lookback=5 lambda=0.35 (injury history is more uniform)\n",
    "    isi_fast = 'ISI_lb5_lamdba_7'\n",
    "    isi_slow = 'ISI_lb5_lamdba_35'\n",
    "    \n",
    "    # defines signals based on decay speeds\n",
    "    if isi_fast in df.columns and isi_slow in df.columns:\n",
    "        df['isi_recency_signal'] = df[isi_fast] - df[isi_slow]\n",
    "        df['injuries_very_recent'] = (df['isi_recency_signal'] > 0.1).astype(int)\n",
    "    \n",
    "    # non-linear transformation\n",
    "    # attempting to see if applying transformations\n",
    "    # to the ISI cols can impact results\n",
    "\n",
    "    # polynomial transformations\n",
    "    # Is there an exponential ISI discount?\n",
    "    df[f'{isi_col}_squared'] = df[isi_col] ** 2\n",
    "    df[f'{isi_col}_sqrt'] = np.sqrt(df[isi_col])\n",
    "    df[f'{isi_col}_log1p'] = np.log1p(df[isi_col])\n",
    "    \n",
    "    # defines thresholds\n",
    "    isi_75 = df[isi_col].quantile(0.75)\n",
    "    isi_25 = df[isi_col].quantile(0.25)\n",
    "    \n",
    "    df['isi_high_burden'] = (df[isi_col] > isi_75).astype(int)\n",
    "    df['isi_clean_history'] = (df[isi_col] <= isi_25).astype(int)\n",
    "\n",
    "    feature_count_end = len(df.columns)\n",
    "    new_features = feature_count_end - feature_count_start\n",
    "    \n",
    "    # page split\n",
    "    print(\"\\n\" + \"=\"*20)\n",
    "    print(f\"New features created: {new_features}\")\n",
    "    print(f\"Total cols: {feature_count_end}\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56848e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def make_preprocessor(numeric_features: List[str], categorical_features: List[str]) -> ColumnTransformer:\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "    \n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_features),\n",
    "            (\"cat\", cat_pipe, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9113fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force CV Grid Search\n",
    "# replicates v1 strucutre\n",
    "\n",
    "def brute_force_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    numeric_features, categorical_features,\n",
    "    variant_name, time_budget_seconds\n",
    "):\n",
    "    \"\"\"Brute force hypterparameters until maximum time runs out\"\"\"\n",
    "    # page split\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"VARIANT: {variant_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Features: {len(numeric_features)} numeric + {len(categorical_features)} categorical\")\n",
    "    print(f\"Time budget: {time_budget_seconds/60:.1f} minutes\")\n",
    "    \n",
    "    # runs proprocessor function created above\n",
    "    preprocessor = make_preprocessor(numeric_features, categorical_features)\n",
    "\n",
    "    # generates param combos\n",
    "    param_keys = list(PARAM_GRID.keys())\n",
    "    param_values = [PARAM_GRID[k] for k in param_keys]\n",
    "    all_combinations = list(product(*param_values))\n",
    "\n",
    "    # shuffle param combos to allow for a randomized grid search\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    np.random.shuffle(all_combinations)\n",
    "\n",
    "    # Results tracker\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    best_mae = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # TimeSeriesSplit for CV (refer to documentation for this)\n",
    "    # CV splits limited to three since the dataset is small (n < 1000 rows)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    for i, combo in enumerate(all_combinations):\n",
    "        # Check time limit\n",
    "        # prints number of combos tested after the time limit expires\n",
    "        elapsed = time.time() - start_time\n",
    "        if elapsed >= time_budget_seconds:\n",
    "            print(f\"\\n[TIME LIMIT] Stopped after {elapsed/60:.1f} minutes, tested {i} combinations\")\n",
    "            break\n",
    "\n",
    "        # create param dict to store results        \n",
    "        params = dict(zip(param_keys, combo))\n",
    "        \n",
    "        try:\n",
    "            base_model = XGBRegressor(\n",
    "                **params,\n",
    "                random_state=RANDOM_STATE,\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_jobs=1,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            pipeline = Pipeline([\n",
    "                (\"pre\", preprocessor),\n",
    "                (\"model\", base_model)\n",
    "            ])\n",
    "            \n",
    "            # log transformation\n",
    "            ttr = TransformedTargetRegressor(\n",
    "                regressor=pipeline,\n",
    "                func=np.log1p,\n",
    "                inverse_func=np.expm1\n",
    "            )\n",
    "            \n",
    "            # CV eval\n",
    "            cv_maes = []\n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                ttr.fit(X_tr, y_tr)\n",
    "                y_pred = ttr.predict(X_val)\n",
    "                mae = mean_absolute_error(y_val, y_pred)\n",
    "                cv_maes.append(mae)\n",
    "            \n",
    "            cv_mae = np.mean(cv_maes)\n",
    "            \n",
    "            # test set eval\n",
    "            ttr.fit(X_train, y_train)\n",
    "            y_test_pred = ttr.predict(X_test)\n",
    "            test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "            # records best results\n",
    "            # prints an update statement on improved results\n",
    "            if test_metrics['MAE'] < best_mae:\n",
    "                best_mae = test_metrics['MAE']\n",
    "                best_params = params\n",
    "                print(f\"\\n[NEW BEST] Iteration {i+1}\")\n",
    "                print(f\"  CV MAE: ${cv_mae:,.0f}\")\n",
    "                print(f\"  Test MAE: ${test_metrics['MAE']:,.0f}\")\n",
    "                print(f\"  Test R²: {test_metrics['R2']:.4f}\")\n",
    "\n",
    "            # append results\n",
    "            results.append({\n",
    "                'cv_mae': cv_mae,\n",
    "                'test_mae': test_metrics['MAE'],\n",
    "                'test_rmse': test_metrics['RMSE'],\n",
    "                'test_r2': test_metrics['R2'],\n",
    "                **params\n",
    "            })\n",
    "\n",
    "            # Displays update every 10 iters\n",
    "            if (i + 1) % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = (i + 1) / elapsed\n",
    "                remaining = time_budget_seconds - elapsed\n",
    "                est_remaining_iters = int(rate * remaining)\n",
    "                print(f\"  [{i+1:4d}] Elapsed: {elapsed/60:.1f}m | Rate: {rate:.1f} iter/s | Est. remaining: {est_remaining_iters}\")\n",
    "\n",
    "        # prevents system crash if a model fails\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Iteration {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    # page split\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPLETED {variant_name}\")\n",
    "    print(f\"Tested {len(results)} combinations in {elapsed_total/60:.1f} minutes\")\n",
    "    print(f\"Best Test MAE: ${best_mae:,.0f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"variant\": variant_name,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_test_mae\": best_mae,\n",
    "        \"n_tested\": len(results),\n",
    "        \"elapsed_seconds\": elapsed_total,\n",
    "        \"all_results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aefcd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MLB CONTRACT REGRESSION V2\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Contracts after filtering: 844\n",
      "\n",
      "Building baseline features...\n",
      "  Batting: 33.2% coverage\n",
      "  Pitching: 39.6% coverage\n",
      "  Defense: 16.6% coverage\n",
      "  Statcast: 16.8% coverage\n",
      "\n",
      "================================================================================\n",
      "V2 FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "1. Creating ISI × Performance/Age interactions...\n",
      "   Created contract length interactions\n",
      "   Total interaction features: 7\n",
      "\n",
      "2. Decomposing injury types (surgery, structural, arm)...\n",
      "   Created 12 injury type features\n",
      "\n",
      "3. Creating position-specific ISI encoding...\n",
      "   Created 5 position features\n",
      "\n",
      "4. Creating ISI temporal trend features...\n",
      "   Created ISI temporal trend features\n",
      "\n",
      "5. Creating nonlinear ISI transformations...\n",
      "   Created 5 nonlinear features\n",
      "\n",
      "================================================================================\n",
      "V2 FEATURE ENGINEERING COMPLETE\n",
      "  New features created: 33\n",
      "  Total columns: 214\n",
      "================================================================================\n",
      "\n",
      "Train: 381 | Test: 191\n",
      "Top 5% cutoff: $22,361,449\n",
      "\n",
      "Baseline features: 79 numeric + 1 categorical\n",
      "\n",
      "================================================================================\n",
      "VARIANTS TO TEST\n",
      "================================================================================\n",
      "BASELINE_V2_RERUN        : 79 numeric features\n",
      "                           Baseline rerun to verify v1 results\n",
      "V2_ALL_FEATURES          : 135 numeric features\n",
      "                           All v2 engineered features\n",
      "V2_INTERACTIONS_ONLY     : 88 numeric features\n",
      "                           ISI interactions only\n",
      "V2_INJURY_TYPES          : 92 numeric features\n",
      "                           Injury type decomposition\n",
      "\n",
      "================================================================================\n",
      "VARIANT: BASELINE_V2_RERUN\n",
      "================================================================================\n",
      "Train: (381, 80), Test: (191, 80)\n",
      "Features: 79 numeric + 1 categorical\n",
      "Time budget: 30.0 minutes\n",
      "Total combinations to test: 614,400\n",
      "\n",
      "[NEW BEST] Iteration 1\n",
      "  CV MAE: $3,203,953\n",
      "  Test MAE: $3,109,889\n",
      "  Test R²: 0.4280\n",
      "\n",
      "[NEW BEST] Iteration 2\n",
      "  CV MAE: $3,115,965\n",
      "  Test MAE: $3,091,311\n",
      "  Test R²: 0.4441\n",
      "\n",
      "[NEW BEST] Iteration 4\n",
      "  CV MAE: $3,165,218\n",
      "  Test MAE: $3,046,162\n",
      "  Test R²: 0.4338\n",
      "\n",
      "[NEW BEST] Iteration 8\n",
      "  CV MAE: $3,182,316\n",
      "  Test MAE: $3,009,573\n",
      "  Test R²: 0.4571\n",
      "  [  10] Elapsed: 0.5m | Rate: 0.3 iter/s | Est. remaining: 606\n",
      "  [  20] Elapsed: 0.8m | Rate: 0.4 iter/s | Est. remaining: 767\n",
      "  [  30] Elapsed: 1.2m | Rate: 0.4 iter/s | Est. remaining: 706\n",
      "  [  40] Elapsed: 1.8m | Rate: 0.4 iter/s | Est. remaining: 629\n",
      "  [  50] Elapsed: 2.1m | Rate: 0.4 iter/s | Est. remaining: 647\n",
      "  [  60] Elapsed: 2.7m | Rate: 0.4 iter/s | Est. remaining: 616\n",
      "  [  70] Elapsed: 3.2m | Rate: 0.4 iter/s | Est. remaining: 586\n",
      "  [  80] Elapsed: 3.5m | Rate: 0.4 iter/s | Est. remaining: 597\n",
      "\n",
      "[NEW BEST] Iteration 90\n",
      "  CV MAE: $3,159,643\n",
      "  Test MAE: $3,004,367\n",
      "  Test R²: 0.4673\n",
      "  [  90] Elapsed: 4.0m | Rate: 0.4 iter/s | Est. remaining: 581\n",
      "  [ 100] Elapsed: 4.4m | Rate: 0.4 iter/s | Est. remaining: 579\n",
      "  [ 110] Elapsed: 5.0m | Rate: 0.4 iter/s | Est. remaining: 554\n",
      "  [ 120] Elapsed: 5.3m | Rate: 0.4 iter/s | Est. remaining: 553\n",
      "  [ 130] Elapsed: 5.7m | Rate: 0.4 iter/s | Est. remaining: 550\n",
      "  [ 140] Elapsed: 6.3m | Rate: 0.4 iter/s | Est. remaining: 530\n",
      "  [ 150] Elapsed: 6.7m | Rate: 0.4 iter/s | Est. remaining: 517\n",
      "  [ 160] Elapsed: 7.5m | Rate: 0.4 iter/s | Est. remaining: 480\n",
      "  [ 170] Elapsed: 8.0m | Rate: 0.4 iter/s | Est. remaining: 468\n",
      "  [ 180] Elapsed: 8.4m | Rate: 0.4 iter/s | Est. remaining: 459\n",
      "  [ 190] Elapsed: 8.9m | Rate: 0.4 iter/s | Est. remaining: 448\n",
      "  [ 200] Elapsed: 9.6m | Rate: 0.3 iter/s | Est. remaining: 428\n",
      "  [ 210] Elapsed: 10.4m | Rate: 0.3 iter/s | Est. remaining: 398\n",
      "  [ 220] Elapsed: 10.8m | Rate: 0.3 iter/s | Est. remaining: 390\n",
      "  [ 230] Elapsed: 11.4m | Rate: 0.3 iter/s | Est. remaining: 373\n",
      "  [ 240] Elapsed: 12.1m | Rate: 0.3 iter/s | Est. remaining: 356\n",
      "  [ 250] Elapsed: 12.6m | Rate: 0.3 iter/s | Est. remaining: 344\n",
      "  [ 260] Elapsed: 13.2m | Rate: 0.3 iter/s | Est. remaining: 330\n",
      "  [ 270] Elapsed: 13.7m | Rate: 0.3 iter/s | Est. remaining: 323\n",
      "  [ 280] Elapsed: 14.1m | Rate: 0.3 iter/s | Est. remaining: 317\n",
      "  [ 290] Elapsed: 14.7m | Rate: 0.3 iter/s | Est. remaining: 302\n",
      "  [ 300] Elapsed: 15.2m | Rate: 0.3 iter/s | Est. remaining: 292\n",
      "  [ 310] Elapsed: 15.7m | Rate: 0.3 iter/s | Est. remaining: 281\n",
      "  [ 320] Elapsed: 16.1m | Rate: 0.3 iter/s | Est. remaining: 276\n",
      "  [ 330] Elapsed: 16.5m | Rate: 0.3 iter/s | Est. remaining: 268\n",
      "\n",
      "[NEW BEST] Iteration 331\n",
      "  CV MAE: $3,140,792\n",
      "  Test MAE: $2,973,342\n",
      "  Test R²: 0.4755\n",
      "  [ 340] Elapsed: 17.1m | Rate: 0.3 iter/s | Est. remaining: 255\n",
      "  [ 350] Elapsed: 17.4m | Rate: 0.3 iter/s | Est. remaining: 252\n",
      "  [ 360] Elapsed: 17.8m | Rate: 0.3 iter/s | Est. remaining: 245\n",
      "  [ 370] Elapsed: 18.2m | Rate: 0.3 iter/s | Est. remaining: 239\n",
      "  [ 380] Elapsed: 18.5m | Rate: 0.3 iter/s | Est. remaining: 236\n",
      "  [ 390] Elapsed: 19.1m | Rate: 0.3 iter/s | Est. remaining: 224\n",
      "  [ 400] Elapsed: 19.7m | Rate: 0.3 iter/s | Est. remaining: 210\n",
      "  [ 410] Elapsed: 20.0m | Rate: 0.3 iter/s | Est. remaining: 203\n",
      "  [ 420] Elapsed: 20.6m | Rate: 0.3 iter/s | Est. remaining: 191\n",
      "  [ 430] Elapsed: 21.0m | Rate: 0.3 iter/s | Est. remaining: 184\n",
      "  [ 440] Elapsed: 21.5m | Rate: 0.3 iter/s | Est. remaining: 172\n",
      "  [ 450] Elapsed: 22.1m | Rate: 0.3 iter/s | Est. remaining: 160\n",
      "  [ 460] Elapsed: 22.5m | Rate: 0.3 iter/s | Est. remaining: 152\n",
      "  [ 470] Elapsed: 22.8m | Rate: 0.3 iter/s | Est. remaining: 147\n",
      "  [ 480] Elapsed: 23.2m | Rate: 0.3 iter/s | Est. remaining: 141\n",
      "  [ 490] Elapsed: 23.6m | Rate: 0.3 iter/s | Est. remaining: 132\n",
      "  [ 500] Elapsed: 24.1m | Rate: 0.3 iter/s | Est. remaining: 121\n",
      "  [ 510] Elapsed: 24.6m | Rate: 0.3 iter/s | Est. remaining: 112\n",
      "  [ 520] Elapsed: 24.9m | Rate: 0.3 iter/s | Est. remaining: 106\n",
      "  [ 530] Elapsed: 25.4m | Rate: 0.3 iter/s | Est. remaining: 95\n",
      "  [ 540] Elapsed: 25.8m | Rate: 0.3 iter/s | Est. remaining: 87\n",
      "  [ 550] Elapsed: 26.3m | Rate: 0.3 iter/s | Est. remaining: 78\n",
      "  [ 560] Elapsed: 26.7m | Rate: 0.3 iter/s | Est. remaining: 69\n",
      "  [ 570] Elapsed: 27.1m | Rate: 0.4 iter/s | Est. remaining: 61\n",
      "  [ 580] Elapsed: 27.4m | Rate: 0.4 iter/s | Est. remaining: 54\n",
      "  [ 590] Elapsed: 27.9m | Rate: 0.4 iter/s | Est. remaining: 45\n",
      "  [ 600] Elapsed: 28.2m | Rate: 0.4 iter/s | Est. remaining: 37\n",
      "  [ 610] Elapsed: 28.6m | Rate: 0.4 iter/s | Est. remaining: 28\n",
      "  [ 620] Elapsed: 29.1m | Rate: 0.4 iter/s | Est. remaining: 19\n",
      "  [ 630] Elapsed: 29.5m | Rate: 0.4 iter/s | Est. remaining: 11\n",
      "  [ 640] Elapsed: 29.9m | Rate: 0.4 iter/s | Est. remaining: 1\n",
      "\n",
      "[TIME LIMIT] Stopped after 30.0 minutes, tested 642 combinations\n",
      "\n",
      "================================================================================\n",
      "COMPLETED BASELINE_V2_RERUN\n",
      "Tested 642 combinations in 30.0 minutes\n",
      "Best Test MAE: $2,973,342\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VARIANT: V2_ALL_FEATURES\n",
      "================================================================================\n",
      "Train: (381, 136), Test: (191, 136)\n",
      "Features: 135 numeric + 1 categorical\n",
      "Time budget: 30.0 minutes\n",
      "Total combinations to test: 614,400\n",
      "\n",
      "[NEW BEST] Iteration 1\n",
      "  CV MAE: $3,256,582\n",
      "  Test MAE: $3,080,261\n",
      "  Test R²: 0.4506\n",
      "\n",
      "[NEW BEST] Iteration 2\n",
      "  CV MAE: $3,165,626\n",
      "  Test MAE: $3,056,456\n",
      "  Test R²: 0.4637\n",
      "\n",
      "[NEW BEST] Iteration 4\n",
      "  CV MAE: $3,265,118\n",
      "  Test MAE: $3,038,455\n",
      "  Test R²: 0.4468\n",
      "\n",
      "[NEW BEST] Iteration 8\n",
      "  CV MAE: $3,242,432\n",
      "  Test MAE: $2,999,537\n",
      "  Test R²: 0.4674\n",
      "  [  10] Elapsed: 0.6m | Rate: 0.3 iter/s | Est. remaining: 513\n",
      "  [  20] Elapsed: 0.9m | Rate: 0.4 iter/s | Est. remaining: 644\n",
      "  [  30] Elapsed: 1.4m | Rate: 0.3 iter/s | Est. remaining: 593\n",
      "  [  40] Elapsed: 2.1m | Rate: 0.3 iter/s | Est. remaining: 530\n",
      "  [  50] Elapsed: 2.5m | Rate: 0.3 iter/s | Est. remaining: 542\n",
      "  [  60] Elapsed: 3.2m | Rate: 0.3 iter/s | Est. remaining: 509\n",
      "  [  70] Elapsed: 3.8m | Rate: 0.3 iter/s | Est. remaining: 476\n",
      "\n",
      "[NEW BEST] Iteration 73\n",
      "  CV MAE: $3,219,451\n",
      "  Test MAE: $2,989,794\n",
      "  Test R²: 0.4597\n",
      "  [  80] Elapsed: 4.3m | Rate: 0.3 iter/s | Est. remaining: 481\n",
      "  [  90] Elapsed: 5.0m | Rate: 0.3 iter/s | Est. remaining: 452\n",
      "  [ 100] Elapsed: 5.5m | Rate: 0.3 iter/s | Est. remaining: 445\n",
      "  [ 110] Elapsed: 6.2m | Rate: 0.3 iter/s | Est. remaining: 423\n",
      "  [ 120] Elapsed: 6.6m | Rate: 0.3 iter/s | Est. remaining: 425\n",
      "  [ 130] Elapsed: 7.1m | Rate: 0.3 iter/s | Est. remaining: 421\n",
      "  [ 140] Elapsed: 7.7m | Rate: 0.3 iter/s | Est. remaining: 403\n",
      "  [ 150] Elapsed: 8.3m | Rate: 0.3 iter/s | Est. remaining: 389\n",
      "  [ 160] Elapsed: 9.2m | Rate: 0.3 iter/s | Est. remaining: 359\n",
      "  [ 170] Elapsed: 9.9m | Rate: 0.3 iter/s | Est. remaining: 347\n",
      "  [ 180] Elapsed: 10.4m | Rate: 0.3 iter/s | Est. remaining: 338\n",
      "  [ 190] Elapsed: 10.8m | Rate: 0.3 iter/s | Est. remaining: 336\n",
      "  [ 200] Elapsed: 11.4m | Rate: 0.3 iter/s | Est. remaining: 328\n",
      "  [ 210] Elapsed: 12.0m | Rate: 0.3 iter/s | Est. remaining: 314\n",
      "  [ 220] Elapsed: 12.4m | Rate: 0.3 iter/s | Est. remaining: 311\n",
      "  [ 230] Elapsed: 13.0m | Rate: 0.3 iter/s | Est. remaining: 302\n",
      "  [ 240] Elapsed: 13.6m | Rate: 0.3 iter/s | Est. remaining: 288\n",
      "  [ 250] Elapsed: 14.3m | Rate: 0.3 iter/s | Est. remaining: 273\n",
      "  [ 260] Elapsed: 15.1m | Rate: 0.3 iter/s | Est. remaining: 256\n",
      "  [ 270] Elapsed: 15.6m | Rate: 0.3 iter/s | Est. remaining: 248\n",
      "\n",
      "[NEW BEST] Iteration 277\n",
      "  CV MAE: $3,198,824\n",
      "  Test MAE: $2,986,001\n",
      "  Test R²: 0.4763\n",
      "  [ 280] Elapsed: 16.1m | Rate: 0.3 iter/s | Est. remaining: 243\n",
      "  [ 290] Elapsed: 16.8m | Rate: 0.3 iter/s | Est. remaining: 228\n",
      "  [ 300] Elapsed: 17.3m | Rate: 0.3 iter/s | Est. remaining: 221\n",
      "  [ 310] Elapsed: 17.8m | Rate: 0.3 iter/s | Est. remaining: 211\n",
      "  [ 320] Elapsed: 18.3m | Rate: 0.3 iter/s | Est. remaining: 205\n",
      "\n",
      "[NEW BEST] Iteration 323\n",
      "  CV MAE: $3,188,514\n",
      "  Test MAE: $2,963,854\n",
      "  Test R²: 0.4798\n",
      "  [ 330] Elapsed: 18.8m | Rate: 0.3 iter/s | Est. remaining: 195\n",
      "  [ 340] Elapsed: 19.6m | Rate: 0.3 iter/s | Est. remaining: 180\n",
      "  [ 350] Elapsed: 20.0m | Rate: 0.3 iter/s | Est. remaining: 175\n",
      "  [ 360] Elapsed: 20.5m | Rate: 0.3 iter/s | Est. remaining: 166\n",
      "  [ 370] Elapsed: 21.0m | Rate: 0.3 iter/s | Est. remaining: 158\n",
      "  [ 380] Elapsed: 21.4m | Rate: 0.3 iter/s | Est. remaining: 153\n",
      "  [ 390] Elapsed: 22.1m | Rate: 0.3 iter/s | Est. remaining: 140\n",
      "  [ 400] Elapsed: 22.8m | Rate: 0.3 iter/s | Est. remaining: 125\n",
      "  [ 410] Elapsed: 23.3m | Rate: 0.3 iter/s | Est. remaining: 117\n",
      "  [ 420] Elapsed: 24.1m | Rate: 0.3 iter/s | Est. remaining: 103\n",
      "  [ 430] Elapsed: 24.5m | Rate: 0.3 iter/s | Est. remaining: 95\n",
      "  [ 440] Elapsed: 25.2m | Rate: 0.3 iter/s | Est. remaining: 83\n",
      "  [ 450] Elapsed: 25.9m | Rate: 0.3 iter/s | Est. remaining: 70\n",
      "  [ 460] Elapsed: 26.5m | Rate: 0.3 iter/s | Est. remaining: 60\n",
      "  [ 470] Elapsed: 26.9m | Rate: 0.3 iter/s | Est. remaining: 54\n",
      "  [ 480] Elapsed: 27.3m | Rate: 0.3 iter/s | Est. remaining: 46\n",
      "  [ 490] Elapsed: 27.9m | Rate: 0.3 iter/s | Est. remaining: 36\n",
      "  [ 500] Elapsed: 28.6m | Rate: 0.3 iter/s | Est. remaining: 25\n",
      "  [ 510] Elapsed: 29.2m | Rate: 0.3 iter/s | Est. remaining: 13\n",
      "  [ 520] Elapsed: 29.7m | Rate: 0.3 iter/s | Est. remaining: 5\n",
      "\n",
      "[TIME LIMIT] Stopped after 30.1 minutes, tested 524 combinations\n",
      "\n",
      "================================================================================\n",
      "COMPLETED V2_ALL_FEATURES\n",
      "Tested 524 combinations in 30.1 minutes\n",
      "Best Test MAE: $2,963,854\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VARIANT: V2_INTERACTIONS_ONLY\n",
      "================================================================================\n",
      "Train: (381, 89), Test: (191, 89)\n",
      "Features: 88 numeric + 1 categorical\n",
      "Time budget: 30.0 minutes\n",
      "Total combinations to test: 614,400\n",
      "\n",
      "[NEW BEST] Iteration 1\n",
      "  CV MAE: $3,232,762\n",
      "  Test MAE: $3,061,833\n",
      "  Test R²: 0.4425\n",
      "\n",
      "[NEW BEST] Iteration 8\n",
      "  CV MAE: $3,255,678\n",
      "  Test MAE: $3,055,150\n",
      "  Test R²: 0.4469\n",
      "\n",
      "[NEW BEST] Iteration 10\n",
      "  CV MAE: $3,172,955\n",
      "  Test MAE: $3,002,791\n",
      "  Test R²: 0.4639\n",
      "  [  10] Elapsed: 0.4m | Rate: 0.4 iter/s | Est. remaining: 672\n",
      "  [  20] Elapsed: 0.7m | Rate: 0.5 iter/s | Est. remaining: 860\n",
      "\n",
      "[NEW BEST] Iteration 28\n",
      "  CV MAE: $3,229,468\n",
      "  Test MAE: $2,979,626\n",
      "  Test R²: 0.4714\n",
      "  [  30] Elapsed: 1.1m | Rate: 0.5 iter/s | Est. remaining: 803\n",
      "\n",
      "[NEW BEST] Iteration 35\n",
      "  CV MAE: $3,232,718\n",
      "  Test MAE: $2,975,542\n",
      "  Test R²: 0.4867\n",
      "  [  40] Elapsed: 1.6m | Rate: 0.4 iter/s | Est. remaining: 728\n",
      "  [  50] Elapsed: 1.9m | Rate: 0.4 iter/s | Est. remaining: 747\n",
      "  [  60] Elapsed: 2.3m | Rate: 0.4 iter/s | Est. remaining: 710\n",
      "  [  70] Elapsed: 2.8m | Rate: 0.4 iter/s | Est. remaining: 675\n",
      "  [  80] Elapsed: 3.1m | Rate: 0.4 iter/s | Est. remaining: 688\n",
      "  [  90] Elapsed: 3.6m | Rate: 0.4 iter/s | Est. remaining: 669\n",
      "  [ 100] Elapsed: 3.9m | Rate: 0.4 iter/s | Est. remaining: 665\n",
      "  [ 110] Elapsed: 4.4m | Rate: 0.4 iter/s | Est. remaining: 635\n",
      "  [ 120] Elapsed: 4.8m | Rate: 0.4 iter/s | Est. remaining: 637\n",
      "  [ 130] Elapsed: 5.1m | Rate: 0.4 iter/s | Est. remaining: 632\n",
      "  [ 140] Elapsed: 5.6m | Rate: 0.4 iter/s | Est. remaining: 607\n",
      "  [ 150] Elapsed: 6.1m | Rate: 0.4 iter/s | Est. remaining: 590\n",
      "  [ 160] Elapsed: 6.8m | Rate: 0.4 iter/s | Est. remaining: 547\n",
      "  [ 170] Elapsed: 7.2m | Rate: 0.4 iter/s | Est. remaining: 535\n",
      "  [ 180] Elapsed: 7.7m | Rate: 0.4 iter/s | Est. remaining: 522\n",
      "  [ 190] Elapsed: 8.0m | Rate: 0.4 iter/s | Est. remaining: 519\n",
      "  [ 200] Elapsed: 8.5m | Rate: 0.4 iter/s | Est. remaining: 507\n",
      "  [ 210] Elapsed: 9.1m | Rate: 0.4 iter/s | Est. remaining: 485\n",
      "  [ 220] Elapsed: 9.4m | Rate: 0.4 iter/s | Est. remaining: 482\n",
      "  [ 230] Elapsed: 9.8m | Rate: 0.4 iter/s | Est. remaining: 470\n",
      "\n",
      "[NEW BEST] Iteration 236\n",
      "  CV MAE: $3,135,119\n",
      "  Test MAE: $2,972,133\n",
      "  Test R²: 0.4618\n",
      "  [ 240] Elapsed: 10.4m | Rate: 0.4 iter/s | Est. remaining: 455\n",
      "  [ 250] Elapsed: 10.9m | Rate: 0.4 iter/s | Est. remaining: 439\n",
      "\n",
      "[NEW BEST] Iteration 260\n",
      "  CV MAE: $3,143,889\n",
      "  Test MAE: $2,970,044\n",
      "  Test R²: 0.4731\n",
      "  [ 260] Elapsed: 11.4m | Rate: 0.4 iter/s | Est. remaining: 422\n",
      "  [ 270] Elapsed: 11.9m | Rate: 0.4 iter/s | Est. remaining: 413\n",
      "  [ 280] Elapsed: 12.2m | Rate: 0.4 iter/s | Est. remaining: 407\n",
      "  [ 290] Elapsed: 12.8m | Rate: 0.4 iter/s | Est. remaining: 389\n",
      "  [ 300] Elapsed: 13.2m | Rate: 0.4 iter/s | Est. remaining: 381\n",
      "  [ 310] Elapsed: 13.7m | Rate: 0.4 iter/s | Est. remaining: 370\n",
      "  [ 320] Elapsed: 14.0m | Rate: 0.4 iter/s | Est. remaining: 364\n",
      "  [ 330] Elapsed: 14.5m | Rate: 0.4 iter/s | Est. remaining: 353\n",
      "  [ 340] Elapsed: 15.1m | Rate: 0.4 iter/s | Est. remaining: 335\n",
      "  [ 350] Elapsed: 15.4m | Rate: 0.4 iter/s | Est. remaining: 331\n",
      "  [ 360] Elapsed: 15.8m | Rate: 0.4 iter/s | Est. remaining: 323\n",
      "  [ 370] Elapsed: 16.2m | Rate: 0.4 iter/s | Est. remaining: 314\n",
      "  [ 380] Elapsed: 16.5m | Rate: 0.4 iter/s | Est. remaining: 310\n",
      "  [ 390] Elapsed: 17.0m | Rate: 0.4 iter/s | Est. remaining: 297\n",
      "  [ 400] Elapsed: 17.6m | Rate: 0.4 iter/s | Est. remaining: 281\n",
      "  [ 410] Elapsed: 18.0m | Rate: 0.4 iter/s | Est. remaining: 274\n",
      "  [ 420] Elapsed: 18.6m | Rate: 0.4 iter/s | Est. remaining: 258\n",
      "  [ 430] Elapsed: 18.9m | Rate: 0.4 iter/s | Est. remaining: 250\n",
      "  [ 440] Elapsed: 19.4m | Rate: 0.4 iter/s | Est. remaining: 239\n",
      "  [ 450] Elapsed: 20.0m | Rate: 0.4 iter/s | Est. remaining: 224\n",
      "  [ 460] Elapsed: 20.5m | Rate: 0.4 iter/s | Est. remaining: 214\n",
      "  [ 470] Elapsed: 20.8m | Rate: 0.4 iter/s | Est. remaining: 207\n",
      "  [ 480] Elapsed: 21.1m | Rate: 0.4 iter/s | Est. remaining: 201\n",
      "  [ 490] Elapsed: 21.6m | Rate: 0.4 iter/s | Est. remaining: 190\n",
      "  [ 500] Elapsed: 22.1m | Rate: 0.4 iter/s | Est. remaining: 177\n",
      "  [ 510] Elapsed: 22.6m | Rate: 0.4 iter/s | Est. remaining: 165\n",
      "  [ 520] Elapsed: 23.0m | Rate: 0.4 iter/s | Est. remaining: 158\n",
      "  [ 530] Elapsed: 23.6m | Rate: 0.4 iter/s | Est. remaining: 145\n",
      "  [ 540] Elapsed: 24.0m | Rate: 0.4 iter/s | Est. remaining: 134\n",
      "  [ 550] Elapsed: 24.5m | Rate: 0.4 iter/s | Est. remaining: 123\n",
      "  [ 560] Elapsed: 25.0m | Rate: 0.4 iter/s | Est. remaining: 111\n",
      "  [ 570] Elapsed: 25.5m | Rate: 0.4 iter/s | Est. remaining: 101\n",
      "  [ 580] Elapsed: 25.9m | Rate: 0.4 iter/s | Est. remaining: 93\n",
      "  [ 590] Elapsed: 26.4m | Rate: 0.4 iter/s | Est. remaining: 81\n",
      "  [ 600] Elapsed: 26.8m | Rate: 0.4 iter/s | Est. remaining: 72\n",
      "  [ 610] Elapsed: 27.2m | Rate: 0.4 iter/s | Est. remaining: 62\n",
      "  [ 620] Elapsed: 27.7m | Rate: 0.4 iter/s | Est. remaining: 52\n",
      "  [ 630] Elapsed: 28.1m | Rate: 0.4 iter/s | Est. remaining: 43\n",
      "  [ 640] Elapsed: 28.5m | Rate: 0.4 iter/s | Est. remaining: 33\n",
      "  [ 650] Elapsed: 29.1m | Rate: 0.4 iter/s | Est. remaining: 19\n",
      "  [ 660] Elapsed: 29.4m | Rate: 0.4 iter/s | Est. remaining: 12\n",
      "  [ 670] Elapsed: 29.9m | Rate: 0.4 iter/s | Est. remaining: 2\n",
      "\n",
      "[TIME LIMIT] Stopped after 30.0 minutes, tested 673 combinations\n",
      "\n",
      "================================================================================\n",
      "COMPLETED V2_INTERACTIONS_ONLY\n",
      "Tested 673 combinations in 30.0 minutes\n",
      "Best Test MAE: $2,970,044\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VARIANT: V2_INJURY_TYPES\n",
      "================================================================================\n",
      "Train: (381, 93), Test: (191, 93)\n",
      "Features: 92 numeric + 1 categorical\n",
      "Time budget: 30.0 minutes\n",
      "Total combinations to test: 614,400\n",
      "\n",
      "[NEW BEST] Iteration 1\n",
      "  CV MAE: $3,265,697\n",
      "  Test MAE: $3,152,397\n",
      "  Test R²: 0.4262\n",
      "\n",
      "[NEW BEST] Iteration 2\n",
      "  CV MAE: $3,187,276\n",
      "  Test MAE: $3,066,323\n",
      "  Test R²: 0.4582\n",
      "\n",
      "[NEW BEST] Iteration 8\n",
      "  CV MAE: $3,291,222\n",
      "  Test MAE: $3,017,588\n",
      "  Test R²: 0.4657\n",
      "  [  10] Elapsed: 0.5m | Rate: 0.3 iter/s | Est. remaining: 599\n",
      "  [  20] Elapsed: 0.8m | Rate: 0.4 iter/s | Est. remaining: 758\n",
      "  [  30] Elapsed: 1.2m | Rate: 0.4 iter/s | Est. remaining: 718\n",
      "  [  40] Elapsed: 1.7m | Rate: 0.4 iter/s | Est. remaining: 657\n",
      "  [  50] Elapsed: 2.1m | Rate: 0.4 iter/s | Est. remaining: 678\n",
      "  [  60] Elapsed: 2.5m | Rate: 0.4 iter/s | Est. remaining: 646\n",
      "  [  70] Elapsed: 3.1m | Rate: 0.4 iter/s | Est. remaining: 614\n",
      "  [  80] Elapsed: 3.4m | Rate: 0.4 iter/s | Est. remaining: 627\n",
      "  [  90] Elapsed: 3.9m | Rate: 0.4 iter/s | Est. remaining: 610\n",
      "  [ 100] Elapsed: 4.2m | Rate: 0.4 iter/s | Est. remaining: 608\n",
      "\n",
      "[NEW BEST] Iteration 110\n",
      "  CV MAE: $3,187,330\n",
      "  Test MAE: $3,014,883\n",
      "  Test R²: 0.4654\n",
      "  [ 110] Elapsed: 4.8m | Rate: 0.4 iter/s | Est. remaining: 583\n",
      "  [ 120] Elapsed: 5.1m | Rate: 0.4 iter/s | Est. remaining: 589\n",
      "  [ 130] Elapsed: 5.4m | Rate: 0.4 iter/s | Est. remaining: 586\n",
      "  [ 140] Elapsed: 6.0m | Rate: 0.4 iter/s | Est. remaining: 559\n",
      "  [ 150] Elapsed: 6.5m | Rate: 0.4 iter/s | Est. remaining: 544\n",
      "  [ 160] Elapsed: 7.2m | Rate: 0.4 iter/s | Est. remaining: 507\n",
      "  [ 170] Elapsed: 7.6m | Rate: 0.4 iter/s | Est. remaining: 496\n",
      "  [ 180] Elapsed: 8.1m | Rate: 0.4 iter/s | Est. remaining: 488\n",
      "  [ 190] Elapsed: 8.4m | Rate: 0.4 iter/s | Est. remaining: 486\n",
      "  [ 200] Elapsed: 8.9m | Rate: 0.4 iter/s | Est. remaining: 476\n",
      "  [ 210] Elapsed: 9.4m | Rate: 0.4 iter/s | Est. remaining: 458\n",
      "  [ 220] Elapsed: 9.8m | Rate: 0.4 iter/s | Est. remaining: 456\n",
      "  [ 230] Elapsed: 10.2m | Rate: 0.4 iter/s | Est. remaining: 444\n",
      "  [ 240] Elapsed: 10.8m | Rate: 0.4 iter/s | Est. remaining: 427\n",
      "  [ 250] Elapsed: 11.3m | Rate: 0.4 iter/s | Est. remaining: 411\n",
      "\n",
      "[NEW BEST] Iteration 260\n",
      "  CV MAE: $3,198,879\n",
      "  Test MAE: $2,986,797\n",
      "  Test R²: 0.4737\n",
      "  [ 260] Elapsed: 12.0m | Rate: 0.4 iter/s | Est. remaining: 388\n",
      "  [ 270] Elapsed: 12.5m | Rate: 0.4 iter/s | Est. remaining: 378\n",
      "  [ 280] Elapsed: 12.9m | Rate: 0.4 iter/s | Est. remaining: 370\n",
      "  [ 290] Elapsed: 13.5m | Rate: 0.4 iter/s | Est. remaining: 352\n",
      "  [ 300] Elapsed: 14.0m | Rate: 0.4 iter/s | Est. remaining: 344\n",
      "  [ 310] Elapsed: 14.4m | Rate: 0.4 iter/s | Est. remaining: 334\n",
      "  [ 320] Elapsed: 14.8m | Rate: 0.4 iter/s | Est. remaining: 328\n",
      "  [ 330] Elapsed: 15.3m | Rate: 0.4 iter/s | Est. remaining: 318\n",
      "  [ 340] Elapsed: 15.9m | Rate: 0.4 iter/s | Est. remaining: 303\n",
      "  [ 350] Elapsed: 16.2m | Rate: 0.4 iter/s | Est. remaining: 299\n",
      "  [ 360] Elapsed: 16.6m | Rate: 0.4 iter/s | Est. remaining: 292\n",
      "  [ 370] Elapsed: 17.0m | Rate: 0.4 iter/s | Est. remaining: 283\n",
      "  [ 380] Elapsed: 17.3m | Rate: 0.4 iter/s | Est. remaining: 280\n",
      "  [ 390] Elapsed: 17.8m | Rate: 0.4 iter/s | Est. remaining: 267\n",
      "  [ 400] Elapsed: 18.4m | Rate: 0.4 iter/s | Est. remaining: 251\n",
      "  [ 410] Elapsed: 18.8m | Rate: 0.4 iter/s | Est. remaining: 244\n",
      "  [ 420] Elapsed: 19.4m | Rate: 0.4 iter/s | Est. remaining: 229\n",
      "  [ 430] Elapsed: 19.8m | Rate: 0.4 iter/s | Est. remaining: 222\n",
      "  [ 440] Elapsed: 20.3m | Rate: 0.4 iter/s | Est. remaining: 211\n",
      "  [ 450] Elapsed: 20.9m | Rate: 0.4 iter/s | Est. remaining: 197\n",
      "  [ 460] Elapsed: 21.3m | Rate: 0.4 iter/s | Est. remaining: 187\n",
      "  [ 470] Elapsed: 21.7m | Rate: 0.4 iter/s | Est. remaining: 180\n",
      "  [ 480] Elapsed: 22.0m | Rate: 0.4 iter/s | Est. remaining: 174\n",
      "  [ 490] Elapsed: 22.5m | Rate: 0.4 iter/s | Est. remaining: 164\n",
      "  [ 500] Elapsed: 23.0m | Rate: 0.4 iter/s | Est. remaining: 153\n",
      "  [ 510] Elapsed: 23.5m | Rate: 0.4 iter/s | Est. remaining: 141\n",
      "  [ 520] Elapsed: 23.8m | Rate: 0.4 iter/s | Est. remaining: 134\n",
      "  [ 530] Elapsed: 24.4m | Rate: 0.4 iter/s | Est. remaining: 120\n",
      "  [ 540] Elapsed: 25.0m | Rate: 0.4 iter/s | Est. remaining: 108\n",
      "  [ 550] Elapsed: 25.5m | Rate: 0.4 iter/s | Est. remaining: 97\n",
      "  [ 560] Elapsed: 26.0m | Rate: 0.4 iter/s | Est. remaining: 85\n",
      "  [ 570] Elapsed: 26.5m | Rate: 0.4 iter/s | Est. remaining: 75\n",
      "  [ 580] Elapsed: 26.9m | Rate: 0.4 iter/s | Est. remaining: 66\n",
      "  [ 590] Elapsed: 27.5m | Rate: 0.4 iter/s | Est. remaining: 54\n",
      "  [ 600] Elapsed: 27.9m | Rate: 0.4 iter/s | Est. remaining: 45\n",
      "  [ 610] Elapsed: 28.3m | Rate: 0.4 iter/s | Est. remaining: 35\n",
      "  [ 620] Elapsed: 28.8m | Rate: 0.4 iter/s | Est. remaining: 25\n",
      "  [ 630] Elapsed: 29.2m | Rate: 0.4 iter/s | Est. remaining: 16\n",
      "  [ 640] Elapsed: 29.7m | Rate: 0.4 iter/s | Est. remaining: 6\n",
      "\n",
      "[TIME LIMIT] Stopped after 30.0 minutes, tested 647 combinations\n",
      "\n",
      "================================================================================\n",
      "COMPLETED V2_INJURY_TYPES\n",
      "Tested 647 combinations in 30.0 minutes\n",
      "Best Test MAE: $2,986,797\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Saved summary to: C:\\Users\\brend\\Desktop\\SABR_research_proposal\\FINAL FILES\\v2 models\\regression\\regression_v2_results.csv\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "             variant  best_test_mae  n_combinations_tested\n",
      "     V2_ALL_FEATURES   2.963854e+06                    524\n",
      "V2_INTERACTIONS_ONLY   2.970044e+06                    673\n",
      "   BASELINE_V2_RERUN   2.973342e+06                    642\n",
      "     V2_INJURY_TYPES   2.986797e+06                    647\n",
      "\n",
      "Saved: C:\\Users\\brend\\Desktop\\SABR_research_proposal\\FINAL FILES\\v2 models\\regression\\regression_v2_results_BASELINE_V2_RERUN_detailed.csv\n",
      "\n",
      "Saved: C:\\Users\\brend\\Desktop\\SABR_research_proposal\\FINAL FILES\\v2 models\\regression\\regression_v2_results_V2_ALL_FEATURES_detailed.csv\n",
      "\n",
      "Saved: C:\\Users\\brend\\Desktop\\SABR_research_proposal\\FINAL FILES\\v2 models\\regression\\regression_v2_results_V2_INTERACTIONS_ONLY_detailed.csv\n",
      "\n",
      "Saved: C:\\Users\\brend\\Desktop\\SABR_research_proposal\\FINAL FILES\\v2 models\\regression\\regression_v2_results_V2_INJURY_TYPES_detailed.csv\n",
      "\n",
      "================================================================================\n",
      "TOTAL TIME: 2.00 hours\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Brute force main run\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # page split\n",
    "    print(\"=\"*20)\n",
    "    print(\"Ruunning Regression V2\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "    # Data loader\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    bat_rates = pd.read_csv(BAT_RATES_PATH, low_memory=False)\n",
    "    pit_rates = pd.read_csv(PIT_RATES_PATH, low_memory=False)\n",
    "    def_stats = pd.read_csv(DEF_STATS_PATH, low_memory=False)\n",
    "    sc_pit = pd.read_csv(STATCAST_PIT_PATH, low_memory=False)\n",
    "    \n",
    "    # Filter contracts\n",
    "    df = df[pd.to_numeric(df[\"years_int\"], errors=\"coerce\") <= MAX_YEARS].copy()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"term_start_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"is_pitcher_flag\"] = df[\"position\"].apply(is_pitcher)\n",
    "    \n",
    "    # displays number of contracts post filtering\n",
    "    print(f\"Contracts after filtering: {len(df)}\")\n",
    "\n",
    "    # defines baseline features and coverage reports\n",
    "    BAT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"PA\", \"bat_rate_reliability\"}\n",
    "    bat_rate_cols = [c for c in bat_rates.columns if c not in BAT_EXCLUDE and not c.endswith(\"_dup\")]\n",
    "    bat_weight_col = \"bat_rate_reliability\" if \"bat_rate_reliability\" in bat_rates.columns else None\n",
    "    \n",
    "    df = add_pre_rate_features(df, bat_rates, rate_cols=bat_rate_cols, weight_col=bat_weight_col, prefix=\"bat\", pre_years=3)\n",
    "    print(f\"  Batting: {df['has_bat_pre'].mean():.1%} coverage\")\n",
    "    \n",
    "    PIT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"IP\", \"TBF\", \"pit_rate_reliability\"}\n",
    "    pit_rate_cols = [c for c in pit_rates.columns if c not in PIT_EXCLUDE and not c.endswith(\"_dup\")]\n",
    "    pit_weight_col = \"pit_rate_reliability\" if \"pit_rate_reliability\" in pit_rates.columns else None\n",
    "    \n",
    "    df = add_pre_rate_features(df, pit_rates, rate_cols=pit_rate_cols, weight_col=pit_weight_col, prefix=\"pit\", pre_years=3)\n",
    "    print(f\"  Pitching: {df['has_pit_pre'].mean():.1%} coverage\")\n",
    "    \n",
    "    def_feature_cols = safe_cols(def_stats, [\"defensive_runs_saved\", \"fielding_percentage\", \"Errors\"])\n",
    "    df = add_pre_panel_features(\n",
    "        df, def_stats,\n",
    "        contract_key_col=\"key_mlbam\", panel_key_col=\"MLBAMID\",\n",
    "        contract_year_col=\"year\", panel_year_col=\"year\",\n",
    "        feature_cols=def_feature_cols,\n",
    "        weight_col=\"Innings_played\" if \"Innings_played\" in def_stats.columns else None,\n",
    "        prefix=\"def\", pre_years=3\n",
    "    )\n",
    "    print(f\"  Defense: {df['has_def_pre'].mean():.1%} coverage\")\n",
    "    \n",
    "    sc_feature_cols = safe_cols(sc_pit, [\"fastball_avg_speed\", \"whiff_percent\", \"hard_hit_percent\", \n",
    "                                          \"barrel_batted_rate\", \"exit_velocity_avg\", \"swing_percent\"])\n",
    "\n",
    "# apply features to the dataset\n",
    "    df = add_pre_panel_features(\n",
    "        df, sc_pit,\n",
    "        contract_key_col=\"key_mlbam\", panel_key_col=\"player_id\",\n",
    "        contract_year_col=\"year\", panel_year_col=\"year\",\n",
    "        feature_cols=sc_feature_cols,\n",
    "        weight_col=\"pa\" if \"pa\" in sc_pit.columns else None,\n",
    "        prefix=\"scpit\", pre_years=3\n",
    "    )\n",
    "    print(f\"  Statcast: {df['has_scpit_pre'].mean():.1%} coverage\")\n",
    "    \n",
    "    # Data integrity\n",
    "    df = dedupe_columns(df)\n",
    "    \n",
    "    # Apply newly created features\n",
    "    df = engineer_v2_features(df)\n",
    "    \n",
    "    # regression setup\n",
    "    df_reg = df[pd.notna(df[REG_TARGET])].copy()\n",
    "    df_reg[REG_TARGET] = pd.to_numeric(df_reg[REG_TARGET], errors=\"coerce\")\n",
    "    df_reg = df_reg[np.isfinite(df_reg[REG_TARGET]) & (df_reg[REG_TARGET] >= 0)].copy()\n",
    "    \n",
    "    train_reg, test_reg = time_split(df_reg, year_col=\"year\")\n",
    "    \n",
    "    # apply AAV autoff\n",
    "    aav_cutoff = train_reg[REG_TARGET].quantile(REMOVE_TOP_PCTL)\n",
    "    train_reg_cut = train_reg[train_reg[REG_TARGET] <= aav_cutoff].copy()\n",
    "    test_reg_cut = test_reg[test_reg[REG_TARGET] <= aav_cutoff].copy()\n",
    "    \n",
    "    # displays size of training and testing datasets (post AAV cutoff)\n",
    "    print(f\"[TOP5%] Cutoff: {aav_cutoff:,.0f}\")\n",
    "    print(f\"[TOP5%] Train: {len(train_reg_cut)} | Test: {len(test_reg_cut)}\")\n",
    "    \n",
    "    # feature lists\n",
    "    BASE_NUMERIC = [\"age_at_signing\", \"years_int\", \"opt_out_flag\", \"year\", \"is_pitcher_flag\"]\n",
    "    BASE_CATEGORICAL = [\"position\", \"qualifying_offer\"]\n",
    "    \n",
    "    cov_suffixes = (\"_pre_seasons\", \"_pre_reliability_sum\", \"_pre_weight_sum\")\n",
    "    generated_cov_feats = [\n",
    "        c for c in df_reg.columns\n",
    "        if c in {\"has_bat_pre\", \"has_pit_pre\", \"has_def_pre\", \"has_scpit_pre\"}\n",
    "        or c.endswith(cov_suffixes)\n",
    "    ]\n",
    "    \n",
    "    PREFIXES = (\"bat_pre_\", \"pit_pre_\", \"def_pre_\", \"scpit_pre_\")\n",
    "    generated_rate_feats = [\n",
    "        c for c in df_reg.columns\n",
    "        if c.startswith(PREFIXES) and c not in generated_cov_feats\n",
    "    ]\n",
    "    \n",
    "    base_numeric_features = unique_list([\n",
    "        c for c in (BASE_NUMERIC + generated_rate_feats + generated_cov_feats)\n",
    "        if c in df_reg.columns\n",
    "    ])\n",
    "    \n",
    "    categorical_features = unique_list([\n",
    "        c for c in BASE_CATEGORICAL\n",
    "        if c in df_reg.columns\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nBaseline features: {len(base_numeric_features)} numeric + {len(categorical_features)} categorical\")\n",
    "    \n",
    "    # Test variants\n",
    "    variants_to_test = [\n",
    "        {\n",
    "            'name': 'BASELINE_V2_RERUN',\n",
    "            'numeric': base_numeric_features,\n",
    "            'categorical': categorical_features,\n",
    "            'description': 'Baseline rerun to verify v1 results'\n",
    "        },\n",
    "        {\n",
    "            'name': 'V2_ALL_FEATURES',\n",
    "            'numeric': unique_list(base_numeric_features + [c for c in df_reg.columns if 'isi' in c.lower() and c not in base_numeric_features]),\n",
    "            'categorical': categorical_features,\n",
    "            'description': 'All v2 engineered features'\n",
    "        },\n",
    "        {\n",
    "            'name': 'V2_INTERACTIONS_ONLY',\n",
    "            'numeric': unique_list(base_numeric_features + [c for c in df_reg.columns if 'isi_x_' in c]),\n",
    "            'categorical': categorical_features,\n",
    "            'description': 'ISI interactions only'\n",
    "        },\n",
    "        {\n",
    "            'name': 'V2_INJURY_TYPES',\n",
    "            'numeric': unique_list(base_numeric_features + [c for c in df_reg.columns if 'burden' in c or c.startswith('had_')]),\n",
    "            'categorical': categorical_features,\n",
    "            'description': 'Injury type decomposition'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for v in variants_to_test:\n",
    "        print(f\"{v['name']:25s}: {len(v['numeric'])} numeric features\")\n",
    "        print(f\"{'':25s}  {v['description']}\")\n",
    "    \n",
    "    # time limit rules\n",
    "    total_seconds = TIME_LIMIT_HOURS * 3600\n",
    "    time_per_variant = total_seconds / len(variants_to_test)\n",
    "    \n",
    "    all_results = []\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    for variant in variants_to_test:\n",
    "        elapsed_total = time.time() - overall_start\n",
    "        if elapsed_total >= total_seconds:\n",
    "            print(f\"\\n[GLOBAL TIME LIMIT] Stopping\")\n",
    "            break\n",
    "        \n",
    "        # filter for available features\n",
    "        available_numeric = [f for f in variant['numeric'] if f in train_reg_cut.columns and f in test_reg_cut.columns]\n",
    "        available_categorical = [f for f in variant['categorical'] if f in train_reg_cut.columns and f in test_reg_cut.columns]\n",
    "        \n",
    "        all_features = available_numeric + available_categorical\n",
    "        \n",
    "        try:\n",
    "            result = brute_force_search(\n",
    "                train_reg_cut[all_features],\n",
    "                train_reg_cut[REG_TARGET],\n",
    "                test_reg_cut[all_features],\n",
    "                test_reg_cut[REG_TARGET],\n",
    "                available_numeric,\n",
    "                available_categorical,\n",
    "                variant['name'],\n",
    "                time_per_variant\n",
    "            )\n",
    "            all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in {variant['name']}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # page split\n",
    "    # Save results\n",
    "    print(\"\\n\" + \"=\"*20)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    # Summary results\n",
    "    if all_results:\n",
    "        summary = pd.DataFrame([\n",
    "            {\n",
    "                \"variant\": r[\"variant\"],\n",
    "                \"best_test_mae\": r[\"best_test_mae\"],\n",
    "                \"n_combinations_tested\": r[\"n_tested\"],\n",
    "                \"elapsed_minutes\": r[\"elapsed_seconds\"] / 60,\n",
    "                \"best_params\": str(r[\"best_params\"])\n",
    "            }\n",
    "            for r in all_results\n",
    "        ])\n",
    "        \n",
    "        summary = summary.sort_values(\"best_test_mae\")\n",
    "        summary.to_csv(OUT_RESULTS, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved summary to: {OUT_RESULTS}\")\n",
    "        # page split\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(summary[[\"variant\", \"best_test_mae\", \"n_combinations_tested\"]].to_string(index=False))\n",
    "        \n",
    "        # saves detailed results for each varaint tested\n",
    "        for r in all_results:\n",
    "            variant_file = OUT_RESULTS.replace(\".csv\", f\"_{r['variant']}_detailed.csv\")\n",
    "            detailed_df = pd.DataFrame(r[\"all_results\"])\n",
    "            detailed_df = detailed_df.sort_values(\"test_mae\")\n",
    "            detailed_df.to_csv(variant_file, index=False)\n",
    "            print(f\"\\nSaved: {variant_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabr_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
