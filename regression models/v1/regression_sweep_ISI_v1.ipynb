{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5069b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLB Contract Regression Sweep, set to a 2 hour time limit\n",
    "Using RandomizedSearchCV to find XGBoost parameters\n",
    "Ensures model runs until params are found or early stopping is implemented\n",
    "\n",
    "Most of the functions are replicated from the baseline model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92828a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "DATA_PATH = r\"contracts_with_isi_v2_SWEEP_WIDE_WITH_KEYS_PLUS_CPI.csv\"\n",
    "BAT_RATES_PATH = r\"batting_rates_by_season.csv\"\n",
    "PIT_RATES_PATH = r\"pitching_rates_by_season.csv\"\n",
    "DEF_STATS_PATH = r\"defensive_stats.csv\"\n",
    "STATCAST_PIT_PATH = r\"statcast_pitching_2015_2025.csv\"\n",
    "\n",
    "# Output path for results and model weights\n",
    "OUT_RESULTS = r\"regression_xgb_2hour_results.csv\"\n",
    "\n",
    "# use this path if pkl file is necessary, csv results save model params\n",
    "OUT_BEST_MODEL = r\"best_xgb_model.pkl\"\n",
    "\n",
    "# establishes time limit for how long the model can run\n",
    "TIME_LIMIT_HOURS = 2.0\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Time split\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]\n",
    "TEST_YEARS = [2024, 2025]\n",
    "\n",
    "# Contract filters\n",
    "# utilzied in config rather than further down, as in original baseline model\n",
    "# max years filtered max contract term\n",
    "# top pctl filters top_n percent of contract AAV\n",
    "MAX_YEARS = 5\n",
    "REMOVE_TOP_PCTL = 0.95\n",
    "\n",
    "# Target\n",
    "REG_TARGET = \"guarantee_real_per_year_2025\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid Search\n",
    "# total of 614,400 potential combinations\n",
    "\n",
    "PARAM_GRID = {\n",
    "    'n_estimators': [500, 1000, 1500, 2000, 2500, 3000],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.03, 0.05],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [3, 5, 7, 10],                  \n",
    "    'subsample': [0.7, 0.8, 0.85, 0.9],                 \n",
    "    'colsample_bytree': [0.7, 0.8, 0.85, 0.9],         \n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0, 3.0],            \n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0],                  \n",
    "    'gamma': [0, 0.1, 0.3, 0.5],                        \n",
    "}\n",
    "\n",
    "\n",
    "# Prints total number of combos\n",
    "total_combos = np.prod(\n",
    "    [len(v) for v in PARAM_GRID.values()]\n",
    ")\n",
    "\n",
    "print(f\"Total possible combos: {total_combos:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# drop dups, preserves order\n",
    "def unique_list(seq):\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "# drop dup col names\n",
    "def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "def is_pitcher(pos) -> int:\n",
    "    PITCHER_PREFIXES = (\"P\", \"SP\", \"RP\", \"RHP\", \"LHP\")\n",
    "    if pd.isna(pos):\n",
    "        return 0\n",
    "    s = str(pos).strip().upper()\n",
    "    # handles pitcher positional variations\n",
    "    return int(s.startswith(PITCHER_PREFIXES) or (\"RHP\" in s) or (\"LHP\" in s))\n",
    "\n",
    "def time_split(df: pd.DataFrame, year_col: str = \"year\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    y = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    train = df[y.isin(TRAIN_YEARS)].copy()\n",
    "    test = df[y.isin(TEST_YEARS)].copy()\n",
    "    return train, test\n",
    "\n",
    "# Data integrity check\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    \n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        raise ValueError(\"No y_true/y_pred pairs found post-filtering.\")\n",
    "    \n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    \n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "def safe_cols(df: pd.DataFrame, col_list: List[str]) -> List[str]:\n",
    "    return [c for c in col_list if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c03044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(f\"Loaded contracts: {len(df)} rows\")\n",
    "\n",
    "df = df[pd.to_numeric(df[\"years_int\"], errors=\"coerce\") <= MAX_YEARS].copy()\n",
    "print(f\"After filtering years_int <= {MAX_YEARS}: {len(df)} rows\")\n",
    "\n",
    "df[\"year\"] = pd.to_numeric(df[\"term_start_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"is_pitcher_flag\"] = df[\"position\"].apply(is_pitcher)\n",
    "\n",
    "bat_rates = pd.read_csv(BAT_RATES_PATH, low_memory=False)\n",
    "pit_rates = pd.read_csv(PIT_RATES_PATH, low_memory=False)\n",
    "def_stats = pd.read_csv(DEF_STATS_PATH, low_memory=False)\n",
    "sc_pit = pd.read_csv(STATCAST_PIT_PATH, low_memory=False)\n",
    "\n",
    "# page split\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING PRE-WINDOW FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def _safe_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _weighted_mean(series: pd.Series, weights: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\").fillna(0.0)\n",
    "    mask = np.isfinite(s) & np.isfinite(w) & (w > 0)\n",
    "    if mask.sum() == 0:\n",
    "        s2 = s[np.isfinite(s)]\n",
    "        return float(s2.mean()) if len(s2) else np.nan\n",
    "    return float(np.average(s[mask], weights=w[mask]))\n",
    "\n",
    "def add_pre_rate_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    season_rates: pd.DataFrame,\n",
    "    *,\n",
    "    rate_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-window aggregate computations for seasons in window\n",
    "    \"\"\"    \n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"key_fangraphs\"] = pd.to_numeric(dfc[\"key_fangraphs\"], errors=\"coerce\")\n",
    "    dfc[\"year\"] = pd.to_numeric(dfc[\"year\"], errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfs = season_rates.copy()\n",
    "    dfs[\"playerId\"] = pd.to_numeric(dfs[\"playerId\"], errors=\"coerce\")\n",
    "    dfs[\"Season\"] = pd.to_numeric(dfs[\"Season\"], errors=\"coerce\")\n",
    "\n",
    "    dfs = _safe_numeric(dfs, rate_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"key_fangraphs\", \"year\"]].merge(\n",
    "        dfs,\n",
    "        left_on=\"key_fangraphs\",\n",
    "        right_on=\"playerId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    m[\"lb_start\"] = m[\"year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"year\"] - 1\n",
    "    m = m[m[\"Season\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"Season\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        rel_sum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_reliability_sum\")\n",
    "        out = out.merge(rel_sum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_reliability_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_reliability_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for rc in rate_cols:\n",
    "        feat_name = f\"{prefix}_pre_{rc}\"\n",
    "        if rc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[rc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[rc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34699ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre panel features\n",
    "\n",
    "def add_pre_panel_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str,\n",
    "    panel_key_col: str,\n",
    "    contract_year_col: str,\n",
    "    panel_year_col: str,\n",
    "    feature_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Generic season panel aggregator (only used for defense and statcast files) \"\"\"\n",
    "    # reduced implementation of data integrity checks since data\n",
    "    # is known to be clean due to success of the baseline model\n",
    "\n",
    "    # temp cols\n",
    "    dfc = contracts.copy()\n",
    "    dfc[contract_key_col] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[contract_year_col] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfp = panel.copy()\n",
    "    dfp[panel_key_col] = pd.to_numeric(dfp[panel_key_col], errors=\"coerce\")\n",
    "    dfp[panel_year_col] = pd.to_numeric(dfp[panel_year_col], errors=\"coerce\")\n",
    "    \n",
    "    dfp = _safe_numeric(dfp, feature_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", contract_key_col, contract_year_col]].merge(\n",
    "        dfp,\n",
    "        left_on=contract_key_col,\n",
    "        right_on=panel_key_col,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_contract\", \"_panel\")\n",
    "    )\n",
    "\n",
    "    # use the panel year column\n",
    "    year_col_in_m = f\"{panel_year_col}_panel\" if f\"{panel_year_col}_panel\" in m.columns else panel_year_col\n",
    "    contract_year_in_m = f\"{contract_year_col}_contract\" if f\"{contract_year_col}_contract\" in m.columns else contract_year_col\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[contract_year_in_m] - pre_years\n",
    "    m[\"lb_end\"] = m[contract_year_in_m] - 1\n",
    "    m = m[m[year_col_in_m].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[year_col_in_m].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    # Weighted sums\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        wsum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_weight_sum\")\n",
    "        out = out.merge(wsum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_weight_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_weight_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Aggregate features where necessary\n",
    "    for fc in feature_cols:\n",
    "        feat_name = f\"{prefix}_pre_{fc}\"\n",
    "        if fc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[fc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[fc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cols to exclude\n",
    "BAT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"PA\", \"bat_rate_reliability\"}\n",
    "bat_rate_cols = [\n",
    "    c for c in bat_rates.columns\n",
    "    if c not in BAT_EXCLUDE\n",
    "    and not c.endswith(\"_dup\")\n",
    "]\n",
    "\n",
    "# cols to exclude\n",
    "PIT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"IP\", \"TBF\", \"pit_rate_reliability\"}\n",
    "pit_rate_cols = [\n",
    "    c for c in pit_rates.columns\n",
    "    if c not in PIT_EXCLUDE\n",
    "    and not c.endswith(\"_dup\")\n",
    "]\n",
    "\n",
    "# Defensive stats feature list\n",
    "def_feature_cols = [c for c in [\"defensive_runs_saved\", \"fielding_percentage\", \"Errors\"] \n",
    "                    if c in def_stats.columns]\n",
    "\n",
    "# Statcast stats feature list\n",
    "sc_feature_cols = [\n",
    "    c for c in [\n",
    "        \"fastball_avg_speed\",\n",
    "        \"whiff_percent\",\n",
    "        \"hard_hit_percent\",\n",
    "        \"barrel_batted_rate\",\n",
    "        \"exit_velocity_avg\",\n",
    "        \"swing_percent\",\n",
    "    ]\n",
    "    if c in sc_pit.columns\n",
    "]\n",
    "\n",
    "# Data integrity\n",
    "bat_weight_col = \"bat_rate_reliability\" if \"bat_rate_reliability\" in bat_rates.columns else None\n",
    "pit_weight_col = \"pit_rate_reliability\" if \"pit_rate_reliability\" in pit_rates.columns else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply features to the dataset using newly created functions above\n",
    "# prints coverage statement for each add_features function\n",
    "\n",
    "df = add_pre_rate_features(\n",
    "    df,\n",
    "    bat_rates,\n",
    "    rate_cols=bat_rate_cols,\n",
    "    weight_col=bat_weight_col,\n",
    "    prefix=\"bat\",\n",
    "    pre_years=3,\n",
    ")\n",
    "print(f\"has_bat_pre: {df['has_bat_pre'].mean():.3f} coverage ({df['has_bat_pre'].sum()}/{len(df)})\")\n",
    "\n",
    "df = add_pre_rate_features(\n",
    "    df,\n",
    "    pit_rates,\n",
    "    rate_cols=pit_rate_cols,\n",
    "    weight_col=pit_weight_col,\n",
    "    prefix=\"pit\",\n",
    "    pre_years=3,\n",
    ")\n",
    "print(f\"has_pit_pre: {df['has_pit_pre'].mean():.3f} coverage ({df['has_pit_pre'].sum()}/{len(df)})\")\n",
    "\n",
    "df = add_pre_panel_features(\n",
    "    contracts=df,\n",
    "    panel=def_stats,\n",
    "    contract_key_col=\"key_mlbam\",\n",
    "    panel_key_col=\"MLBAMID\",\n",
    "    contract_year_col=\"year\",\n",
    "    panel_year_col=\"year\",\n",
    "    feature_cols=def_feature_cols,\n",
    "    weight_col=\"Innings_played\" if \"Innings_played\" in def_stats.columns else None,\n",
    "    prefix=\"def\",\n",
    "    pre_years=3,\n",
    ")\n",
    "print(f\"has_def_pre: {df['has_def_pre'].mean():.3f} coverage ({df['has_def_pre'].sum()}/{len(df)})\")\n",
    "\n",
    "df = add_pre_panel_features(\n",
    "    contracts=df,\n",
    "    panel=sc_pit,\n",
    "    contract_key_col=\"key_mlbam\",\n",
    "    panel_key_col=\"player_id\",\n",
    "    contract_year_col=\"year\",\n",
    "    panel_year_col=\"year\",\n",
    "    feature_cols=sc_feature_cols,\n",
    "    weight_col=\"pa\" if \"pa\" in sc_pit.columns else None,\n",
    "    prefix=\"scpit\",\n",
    "    pre_years=3,\n",
    ")\n",
    "print(f\"has_scpit_pre: {df['has_scpit_pre'].mean():.3f} coverage ({df['has_scpit_pre'].sum()}/{len(df)})\")\n",
    "\n",
    "df = dedupe_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_NUMERIC = [\"age_at_signing\", \"years_int\", \"opt_out_flag\", \"year\", \"is_pitcher_flag\"]\n",
    "BASE_CATEGORICAL = [\"position\", \"qualifying_offer\"]\n",
    "\n",
    "cov_suffixes = (\"_pre_seasons\", \"_pre_reliability_sum\", \"_pre_weight_sum\")\n",
    "generated_cov_feats = [\n",
    "    c for c in df.columns\n",
    "    if c in {\"has_bat_pre\", \"has_pit_pre\", \"has_def_pre\", \"has_scpit_pre\"}\n",
    "    or c.endswith(cov_suffixes)\n",
    "]\n",
    "\n",
    "PREFIXES = (\"bat_pre_\", \"pit_pre_\", \"def_pre_\", \"scpit_pre_\")\n",
    "generated_rate_feats = [\n",
    "    c for c in df.columns\n",
    "    if c.startswith(PREFIXES) and c not in generated_cov_feats\n",
    "]\n",
    "\n",
    "# builds final feature lists\n",
    "base_numeric_features = unique_list([\n",
    "    c for c in (BASE_NUMERIC + generated_rate_feats + generated_cov_feats)\n",
    "    if c in df.columns\n",
    "])\n",
    "\n",
    "categorical_features = unique_list([\n",
    "    c for c in BASE_CATEGORICAL\n",
    "    if c in df.columns\n",
    "])\n",
    "\n",
    "# print count of numeric and categorical features\n",
    "print(f\"\\n[INFO] Base numeric features: {len(base_numeric_features)}\")\n",
    "print(f\"[INFO] Categorical features: {len(categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32722ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISI inclusion\n",
    "\n",
    "# ISI variants\n",
    "# used to test different lambdas (weight decay) and lookback periods\n",
    "ISI_VARIANTS = [\n",
    "    {\"name\": \"lb3_l35\", \"isi\": \"ISI_lb3_lamdba_35\"},\n",
    "    {\"name\": \"lb3_l50\", \"isi\": \"ISI_lb3_lamdba_5\"},\n",
    "    {\"name\": \"lb3_l70\", \"isi\": \"ISI_lb3_lamdba_7\"},\n",
    "    {\"name\": \"lb5_l35\", \"isi\": \"ISI_lb5_lamdba_35\"},\n",
    "    {\"name\": \"lb5_l50\", \"isi\": \"ISI_lb5_lamdba_5\"},\n",
    "    {\"name\": \"lb5_l70\", \"isi\": \"ISI_lb5_lamdba_7\"},\n",
    "]\n",
    "\n",
    "def isi_core_cols(df: pd.DataFrame, isi_col: str) -> List[str]:\n",
    "    suffix = isi_col.replace(\"ISI\", \"\", 1)\n",
    "    candidates = [\n",
    "        isi_col,\n",
    "        f\"isi_full_window_flag{suffix}\",\n",
    "        f\"isi_window_seasons_avail{suffix}\",\n",
    "        f\"any_surgery_flag{suffix}\",\n",
    "        f\"any_structural_flag{suffix}\",\n",
    "        f\"any_tier3_plus_flag{suffix}\",\n",
    "        f\"any_tier_3plus_flag{suffix}\",\n",
    "    ]\n",
    "    return safe_cols(df, candidates)\n",
    "\n",
    "df_reg = df[pd.notna(df[REG_TARGET])].copy()\n",
    "train_reg, test_reg = time_split(df_reg, year_col=\"year\")\n",
    "\n",
    "# apply AAV autoff\n",
    "aav_cutoff = train_reg[REG_TARGET].quantile(REMOVE_TOP_PCTL)\n",
    "train_reg_cut = train_reg[train_reg[REG_TARGET] <= aav_cutoff].copy()\n",
    "test_reg_cut = test_reg[test_reg[REG_TARGET] <= aav_cutoff].copy()\n",
    "\n",
    "# displays size of training and testing datasets (post AAV cutoff)\n",
    "print(f\"[TOP5%] Cutoff: {aav_cutoff:,.0f}\")\n",
    "print(f\"[TOP5%] Train: {len(train_reg_cut)} | Test: {len(test_reg_cut)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def make_preprocessor(numeric_features: List[str], categorical_features: List[str]) -> ColumnTransformer:\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "    \n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_features),\n",
    "            (\"cat\", cat_pipe, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force CV Grid Search\n",
    "\n",
    "def brute_force_search(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    numeric_features, categorical_features,\n",
    "    variant_name, time_budget_seconds\n",
    "):\n",
    "    \"\"\"Brute force hypterparameters until maximum time runs out\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"VARIANT: {variant_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Features: {len(numeric_features)} numeric + {len(categorical_features)} categorical\")\n",
    "    print(f\"Time budget: {time_budget_seconds/60:.1f} minutes\")\n",
    "    \n",
    "    # runs proprocessor function created above\n",
    "    preprocessor = make_preprocessor(numeric_features, categorical_features)\n",
    "    \n",
    "    # generates param combos\n",
    "    param_keys = list(PARAM_GRID.keys())\n",
    "    param_values = [PARAM_GRID[k] for k in param_keys]\n",
    "    all_combinations = list(product(*param_values))\n",
    "\n",
    "    # shuffle param combos to allow for a randomized grid search\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    np.random.shuffle(all_combinations)\n",
    "        \n",
    "    # Results tracker\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    best_mae = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # TimeSeriesSplit for CV (refer to documentation for this)\n",
    "    # CV splits limited to three since the dataset is small (n < 1000 rows)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    for i, combo in enumerate(all_combinations):\n",
    "        # Check time limit\n",
    "        # prints number of combos tested after the time limit expires\n",
    "        elapsed = time.time() - start_time\n",
    "        if elapsed >= time_budget_seconds:\n",
    "            print(f\"\\n[TIME LIMIT] Stopped after {elapsed/60:.1f} minutes, tested {i} combinations\")\n",
    "            break\n",
    "        \n",
    "        # create param dict to store results\n",
    "        params = dict(zip(param_keys, combo))\n",
    "        \n",
    "        try:\n",
    "            base_model = XGBRegressor(\n",
    "                **params,\n",
    "                random_state=RANDOM_STATE,\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_jobs=1,\n",
    "                verbosity=0\n",
    "            )\n",
    "            \n",
    "            pipeline = Pipeline([\n",
    "                (\"pre\", preprocessor),\n",
    "                (\"model\", base_model)\n",
    "            ])\n",
    "            \n",
    "            # log transformation\n",
    "            ttr = TransformedTargetRegressor(\n",
    "                regressor=pipeline,\n",
    "                func=np.log1p,\n",
    "                inverse_func=np.expm1\n",
    "            )\n",
    "            \n",
    "            # CV eval\n",
    "            cv_maes = []\n",
    "            for train_idx, val_idx in tscv.split(X_train):\n",
    "                X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "                y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                ttr.fit(X_tr, y_tr)\n",
    "                y_pred = ttr.predict(X_val)\n",
    "                mae = mean_absolute_error(y_val, y_pred)\n",
    "                cv_maes.append(mae)\n",
    "            \n",
    "            cv_mae = np.mean(cv_maes)\n",
    "            \n",
    "            # test set eval\n",
    "            ttr.fit(X_train, y_train)\n",
    "            y_test_pred = ttr.predict(X_test)\n",
    "            test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "            \n",
    "            # records best results\n",
    "            # prints an update statement on improved results\n",
    "            if test_metrics['MAE'] < best_mae:\n",
    "                best_mae = test_metrics['MAE']\n",
    "                best_params = params\n",
    "                print(f\"\\n[NEW BEST] Iteration {i+1}/{len(all_combinations)}\")\n",
    "                print(f\"  CV MAE: ${cv_mae:,.0f}\")\n",
    "                print(f\"  Test MAE: ${test_metrics['MAE']:,.0f}\")\n",
    "                print(f\"  Test RÂ²: {test_metrics['R2']:.4f}\")\n",
    "                print(f\"  Params: {params}\")\n",
    "            \n",
    "            # append results\n",
    "            results.append({\n",
    "                'cv_mae': cv_mae,\n",
    "                'test_mae': test_metrics['MAE'],\n",
    "                'test_rmse': test_metrics['RMSE'],\n",
    "                'test_r2': test_metrics['R2'],\n",
    "                **params\n",
    "            })\n",
    "            \n",
    "            # Displays update every 10 iters\n",
    "            if (i + 1) % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = (i + 1) / elapsed\n",
    "                remaining = time_budget_seconds - elapsed\n",
    "                est_remaining_iters = int(rate * remaining)\n",
    "                print(f\"  [{i+1:4d}] Elapsed: {elapsed/60:.1f}m | Rate: {rate:.1f} iter/s | Est. remaining: {est_remaining_iters} iters\")\n",
    "        \n",
    "        # prevents system crash if a model fails\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Iteration {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPLETED {variant_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Tested {len(results)} combinations in {elapsed_total/60:.1f} minutes\")\n",
    "    print(f\"Best Test MAE: ${best_mae:,.0f}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "    \n",
    "    return {\n",
    "        \"variant\": variant_name,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_test_mae\": best_mae,\n",
    "        \"n_tested\": len(results),\n",
    "        \"elapsed_seconds\": elapsed_total,\n",
    "        \"all_results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fde320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force main run\n",
    "\n",
    "# time limit rules\n",
    "total_seconds = TIME_LIMIT_HOURS * 3600\n",
    "n_variants = 1 + len(ISI_VARIANTS)\n",
    "time_per_variant = total_seconds / n_variants\n",
    "\n",
    "all_variant_results = []\n",
    "overall_start = time.time()\n",
    "\n",
    "# BASELINE model params\n",
    "baseline_features = [f for f in base_numeric_features + categorical_features \n",
    "                     if f in train_reg_cut.columns and f in test_reg_cut.columns]\n",
    "baseline_numeric = [f for f in base_numeric_features if f in train_reg_cut.columns]\n",
    "\n",
    "# tests baseline model (attempts to improve original baseline results)\n",
    "try:\n",
    "    result = brute_force_search(\n",
    "        train_reg_cut[baseline_features],\n",
    "        train_reg_cut[REG_TARGET],\n",
    "        test_reg_cut[baseline_features],\n",
    "        test_reg_cut[REG_TARGET],\n",
    "        baseline_numeric,\n",
    "        categorical_features,\n",
    "        \"BASELINE\",\n",
    "        time_per_variant\n",
    "    )\n",
    "    all_variant_results.append(result)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in BASELINE: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# tests ISI variant models\n",
    "for isi_var in ISI_VARIANTS:\n",
    "    elapsed_total = time.time() - overall_start\n",
    "    if elapsed_total >= total_seconds:\n",
    "        print(f\"\\n[GLOBAL TIME LIMIT] Stopping after {elapsed_total/3600:.2f} hours\")\n",
    "        break\n",
    "    \n",
    "    isi_col = isi_var[\"isi\"]\n",
    "    variant_name = f\"ISI_CORE_{isi_var['name']}\"\n",
    "    \n",
    "    isi_cols = isi_core_cols(df_reg, isi_col)\n",
    "    if not isi_cols:\n",
    "        print(f\"[SKIP] {variant_name}: ISI columns not found\")\n",
    "        continue\n",
    "    \n",
    "    numeric_feats_isi = unique_list(base_numeric_features + isi_cols)\n",
    "    all_feats_isi = numeric_feats_isi + categorical_features\n",
    "    \n",
    "    available_features = [f for f in all_feats_isi if f in train_reg_cut.columns and f in test_reg_cut.columns]\n",
    "    available_numeric = [f for f in numeric_feats_isi if f in train_reg_cut.columns]\n",
    "    \n",
    "    try:\n",
    "        result = brute_force_search(\n",
    "            train_reg_cut[available_features],\n",
    "            train_reg_cut[REG_TARGET],\n",
    "            test_reg_cut[available_features],\n",
    "            test_reg_cut[REG_TARGET],\n",
    "            available_numeric,\n",
    "            categorical_features,\n",
    "            variant_name,\n",
    "            time_per_variant\n",
    "        )\n",
    "        all_variant_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in {variant_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# page split\n",
    "# saves results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not all_variant_results:\n",
    "    print(\"[WARNING] No results to save\")\n",
    "else:\n",
    "    # Summary results\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            \"variant\": r[\"variant\"],\n",
    "            \"best_test_mae\": r[\"best_test_mae\"],\n",
    "            \"n_combinations_tested\": r[\"n_tested\"],\n",
    "            \"elapsed_minutes\": r[\"elapsed_seconds\"] / 60,\n",
    "            \"best_params\": str(r[\"best_params\"])\n",
    "        }\n",
    "        for r in all_variant_results\n",
    "    ])\n",
    "    \n",
    "    summary = summary.sort_values(\"best_test_mae\")\n",
    "    summary.to_csv(OUT_RESULTS, index=False)\n",
    "    \n",
    "    print(f\"\\nSaved summary to: {OUT_RESULTS}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary[[\"variant\", \"best_test_mae\", \"n_combinations_tested\", \"elapsed_minutes\"]].to_string(index=False))\n",
    "    \n",
    "    # saves detailed results for each varaint tested\n",
    "    for r in all_variant_results:\n",
    "        variant_file = OUT_RESULTS.replace(\".csv\", f\"_{r['variant']}_detailed.csv\")\n",
    "        detailed_df = pd.DataFrame(r[\"all_results\"])\n",
    "        detailed_df = detailed_df.sort_values(\"test_mae\")\n",
    "        detailed_df.to_csv(variant_file, index=False)\n",
    "        print(f\"Rsults saved: {variant_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabr_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
