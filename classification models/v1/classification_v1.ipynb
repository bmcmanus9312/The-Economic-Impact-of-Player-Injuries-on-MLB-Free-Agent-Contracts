{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309aa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import (StratifiedKFold, cross_val_score, cross_val_predict)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# using @dataclass config for simplicity\n",
    "@dataclass\n",
    "class Config:\n",
    "        \n",
    "    DATA_PATH = r\"contracts_with_isi_v2_SWEEP_WIDE_WITH_KEYS_PLUS_CPI.csv\"\n",
    "\n",
    "    BAT_RATES_PATH = r\"batting_rates_by_season.csv\"\n",
    "    PIT_RATES_PATH = r\"pitching_rates_by_season.csv\"\n",
    "    DEF_STATS_PATH = r\"defensive_stats.csv\"\n",
    "    STATCAST_PIT_PATH = r\"statcast_pitching_2015_2025.csv\"\n",
    "\n",
    "    # WAR folder directory (contains war files from 2010-2025)\n",
    "    WAR_DIR = r\"war_rankings_raw\\war\"\n",
    "\n",
    "    # output file\n",
    "    OUTPUT_DIR: str = r\"v2_optimized\"\n",
    "    \n",
    "    # Time limits\n",
    "    # stops if no improvements after 30 min\n",
    "    MAX_RUNTIME_HOURS: float = 2.0\n",
    "    EARLY_STOP_NO_IMPROVE_MINUTES: int = 30\n",
    "    \n",
    "    # CV setup\n",
    "    # optuna trials set to 10k to ensure ennough iters for time limit\n",
    "    N_CV_FOLDS: int = 5\n",
    "    N_OPTUNA_TRIALS: int = 10000\n",
    "    \n",
    "    # setup is IMMUTABLE, WILL LOOK DIFFERENT THAN BASELINE\n",
    "    # this config should prevent any issues with optuna\n",
    "    # Data splits\n",
    "    TRAIN_YEARS: set = field(default_factory=lambda: {2020, 2021, 2022, 2023})\n",
    "    TEST_YEARS: set = field(default_factory=lambda: {2024, 2025})\n",
    "    \n",
    "    # Targets\n",
    "    WAR_LOSS_THRESHOLDS: List[float] = field(default_factory=lambda: [0.5, 1.0, 1.5])\n",
    "    POST_YEARS_OPTIONS: List[int] = field(default_factory=lambda: [1, 3])\n",
    "    PRIMARY_WAR_THRESHOLD: float = 1.0  # Focus optimization here\n",
    "    PRIMARY_POST_YEARS: int = 1\n",
    "    \n",
    "    PRE_YEARS: int = 3\n",
    "    MAX_CONTRACT_YEARS: int = 5\n",
    "    \n",
    "    # ISI configs for lookback and lambda\n",
    "    ISI_LOOKBACKS: List[int] = field(default_factory=lambda: [3, 5])\n",
    "    ISI_LAMBDAS: List[str] = field(default_factory=lambda: [\"35\", \"5\", \"7\"])\n",
    "    \n",
    "    # best results\n",
    "    TOP_N_MODELS: int = 10\n",
    "    \n",
    "    RANDOM_STATE: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77826f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# drop dups, preserves order\n",
    "def unique_list(seq):\n",
    "    \"\"\"Preserve order, drop duplicates.\"\"\"\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "# drop dup col names\n",
    "def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "PITCHER_PREFIXES = (\"P\", \"SP\", \"RP\", \"RHP\", \"LHP\")\n",
    "\n",
    "def is_pitcher(pos) -> int:\n",
    "    if pd.isna(pos):\n",
    "        return 0\n",
    "    s = str(pos).strip().upper()\n",
    "    # handles pitcher positional variations\n",
    "    return int(s.startswith(PITCHER_PREFIXES) or (\"RHP\" in s) or (\"LHP\" in s))\n",
    "\n",
    "\n",
    "def safe_cols(df: pd.DataFrame, cols: List[str]) -> List[str]:\n",
    "    \"\"\"Return only columns that exist in df.\"\"\"\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "\n",
    "def time_split(df: pd.DataFrame, train_years: set, test_years: set, year_col: str = \"year\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data by year.\"\"\"\n",
    "    y = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    train = df[y.isin(train_years)].copy()\n",
    "    test = df[y.isin(test_years)].copy()\n",
    "    return train, test\n",
    "\n",
    "def _safe_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _weighted_mean(series: pd.Series, weights: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\").fillna(0.0)\n",
    "    mask = np.isfinite(s) & np.isfinite(w) & (w > 0)\n",
    "    if mask.sum() == 0:\n",
    "        s2 = s[np.isfinite(s)]\n",
    "        return float(s2.mean()) if len(s2) else np.nan\n",
    "    return float(np.average(s[mask], weights=w[mask]))\n",
    "\n",
    "def add_pre_rate_features(\n",
    "    contracts: pd.DataFrame, season_rates: pd.DataFrame, *,\n",
    "    rate_cols: List[str], weight_col: Optional[str], prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"key_fangraphs\"] = pd.to_numeric(dfc.get(\"key_fangraphs\"), errors=\"coerce\")\n",
    "    dfc[\"year\"] = pd.to_numeric(dfc.get(\"year\"), errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfs = season_rates.copy()\n",
    "    dfs[\"playerId\"] = pd.to_numeric(dfs.get(\"playerId\"), errors=\"coerce\")\n",
    "    dfs[\"Season\"] = pd.to_numeric(dfs.get(\"Season\"), errors=\"coerce\")\n",
    "    dfs = _safe_numeric(dfs, rate_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"key_fangraphs\", \"year\"]].merge(\n",
    "        dfs, left_on=\"key_fangraphs\", right_on=\"playerId\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[\"year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"year\"] - 1\n",
    "    m = m[m[\"Season\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"Season\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        rel_sum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_reliability_sum\")\n",
    "        out = out.merge(rel_sum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_reliability_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_reliability_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for rc in rate_cols:\n",
    "        feat_name = f\"{prefix}_pre_{rc}\"\n",
    "        if rc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[rc], g[weight_col]), include_groups=False).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[rc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre panel features\n",
    "# reduction of 'if' statements since baseline confirmed previous validity\n",
    "\n",
    "def add_pre_panel_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str,\n",
    "    panel_key_col: str,\n",
    "    contract_year_col: str,\n",
    "    panel_year_col: str,\n",
    "    feature_cols: List[str],\n",
    "    weight_col: Optional[str],\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Generic season panel aggregator (only used for defense and statcast files) \"\"\"\n",
    "    # Data integrity\n",
    "    # Missing value check\n",
    "    missing_contract = [c for c in [contract_key_col, contract_year_col] if c not in contracts.columns]\n",
    "    missing_panel = [c for c in [panel_key_col, panel_year_col] if c not in panel.columns]\n",
    "    if missing_contract:\n",
    "        raise KeyError(f\"contracts missing: {missing_contract}\")\n",
    "    if missing_panel:\n",
    "        raise KeyError(f\"panel missing: {missing_panel}\")\n",
    "\n",
    "    out = contracts.copy()\n",
    "    out[\"_row_id\"] = np.arange(len(out), dtype=int)\n",
    "\n",
    "    # temp cols\n",
    "    dfc = out[[\"_row_id\", contract_key_col, contract_year_col]].copy()\n",
    "    dfc[\"_contract_key\"] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[\"_contract_year\"] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "\n",
    "    # returns necessary cols only\n",
    "    keep_feats = [c for c in feature_cols if c in panel.columns]\n",
    "    keep_cols = [panel_key_col, panel_year_col] + keep_feats\n",
    "    if weight_col and weight_col in panel.columns and weight_col not in keep_cols:\n",
    "        keep_cols.append(weight_col)\n",
    "\n",
    "    p = panel[keep_cols].copy()\n",
    "    p[\"_panel_key\"] = pd.to_numeric(p[panel_key_col], errors=\"coerce\")\n",
    "    p[\"_panel_year\"] = pd.to_numeric(p[panel_year_col], errors=\"coerce\")\n",
    "\n",
    "    for c in keep_feats:\n",
    "        p[c] = pd.to_numeric(p[c], errors=\"coerce\")\n",
    "    if weight_col and weight_col in p.columns:\n",
    "        p[weight_col] = pd.to_numeric(p[weight_col], errors=\"coerce\")\n",
    "\n",
    "    merge_cols = [\"_panel_key\", \"_panel_year\"] + keep_feats + ([weight_col] if (weight_col and weight_col in p.columns) else [])\n",
    "    m = dfc[[\"_row_id\", \"_contract_key\", \"_contract_year\"]].merge(\n",
    "        p[merge_cols], left_on=\"_contract_key\", right_on=\"_panel_key\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[\"_contract_year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"_contract_year\"] - 1\n",
    "    m = m[m[\"_panel_year\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"_panel_year\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, on=\"_row_id\", how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    # Weighted sums\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        wsum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_weight_sum\")\n",
    "        out = out.merge(wsum, on=\"_row_id\", how=\"left\")\n",
    "        out[f\"{prefix}_pre_weight_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_weight_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "        \n",
    "        # Aggregate features where necessary\n",
    "        w = pd.to_numeric(m[weight_col], errors=\"coerce\").fillna(0.0)\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            x = pd.to_numeric(m[fc], errors=\"coerce\")\n",
    "            num = (x * w).groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            den = w.groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            wmean = (num / den).replace([np.inf, -np.inf], np.nan).rename(feat_name)\n",
    "            out = out.merge(wmean, on=\"_row_id\", how=\"left\")\n",
    "    else:\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            mean = m.groupby(\"_row_id\")[fc].mean().rename(feat_name)\n",
    "            out = out.merge(mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Utilize WAR Statistics\n",
    "\n",
    "# Data loader & Data Integrity Check\n",
    "def load_war_panel(\n",
    "        war_dir: str, start_year: int = 2010, end_year: int = 2025\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    dfs = []\n",
    "    for y in range(start_year, end_year + 1):\n",
    "        fp = os.path.join(war_dir, f\"war_{y}.csv\")\n",
    "        try:\n",
    "            d = pd.read_csv(fp)\n",
    "            d[\"Season\"] = pd.to_numeric(d.get(\"Season\", y), errors=\"coerce\")\n",
    "            dfs.append(d)\n",
    "        # using pass to prevent file crashing since players are not expected\n",
    "        # to be found in all war files\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    # Raises error if directory path is invalid\n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(f\"No WAR files found in {war_dir}\")\n",
    "    \n",
    "    # combines war files into a single concatenated df\n",
    "    war = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if \"PlayerId\" not in war.columns:\n",
    "        raise KeyError(\"WAR df missing 'PlayerId' col\")\n",
    "\n",
    "    if \"Total WAR\" not in war.columns:\n",
    "        # multiple candidates since term can change over time\n",
    "        cands = [c for c in war.columns if c.strip().lower() in {\"total war\", \"war\", \"total_war\"}]\n",
    "        if cands:\n",
    "            war = war.rename(columns={cands[0]: \"Total WAR\"})\n",
    "        else:\n",
    "            raise KeyError(\"WAR df missing 'Total WAR' col\")\n",
    "\n",
    "    # Data Integrity\n",
    "    war[\"PlayerId\"] = pd.to_numeric(war[\"PlayerId\"], errors=\"coerce\")\n",
    "    war[\"Season\"] = pd.to_numeric(war[\"Season\"], errors=\"coerce\")\n",
    "    war[\"Total WAR\"] = pd.to_numeric(war[\"Total WAR\"], errors=\"coerce\")\n",
    "    war = war.dropna(subset=[\"PlayerId\", \"Season\"]).copy()\n",
    "    \n",
    "    return war"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20789d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_war_windows(\n",
    "    contracts: pd.DataFrame, war_panel: pd.DataFrame, *,\n",
    "    key_col: str = \"key_fangraphs\", year_col: str = \"year\",\n",
    "    pre_years: int = 3, post_years: int = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"adds pre/post WAR aggregated stats\"\"\"\n",
    "\n",
    "    # Following rows are daata type conversions\n",
    "    # should be unnecessary, but prevents future errors\n",
    "    out = contracts.copy()\n",
    "    out[\"_row_id\"] = np.arange(len(out), dtype=int)\n",
    "\n",
    "    c = out[[\"_row_id\", key_col, year_col]].copy()\n",
    "    c[\"_pid\"] = pd.to_numeric(c[key_col], errors=\"coerce\")\n",
    "    c[\"_yr\"] = pd.to_numeric(c[year_col], errors=\"coerce\")\n",
    "\n",
    "    w = war_panel[[\"PlayerId\", \"Season\", \"Total WAR\"]].copy()\n",
    "    w = w.rename(columns={\"PlayerId\": \"_pid\", \"Season\": \"_season\", \"Total WAR\": \"_war\"})\n",
    "    w[\"_pid\"] = pd.to_numeric(w[\"_pid\"], errors=\"coerce\")\n",
    "    w[\"_season\"] = pd.to_numeric(w[\"_season\"], errors=\"coerce\")\n",
    "    w[\"_war\"] = pd.to_numeric(w[\"_war\"], errors=\"coerce\")\n",
    "\n",
    "    m = c.merge(w, on=\"_pid\", how=\"left\")\n",
    "\n",
    "    # Pre: [Y-pre, Y-1]\n",
    "    m[\"pre_start\"] = m[\"_yr\"] - pre_years\n",
    "    m[\"pre_end\"] = m[\"_yr\"] - 1\n",
    "    pre = m[m[\"_season\"].between(m[\"pre_start\"], m[\"pre_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    pre_sum = pre.groupby(\"_row_id\")[\"_war\"].sum(min_count=1).rename(\"war_pre_sum\")\n",
    "    pre_n = pre.groupby(\"_row_id\")[\"_season\"].nunique().rename(\"war_pre_n\")\n",
    "    pre_mean = (pre_sum / pre_n).rename(\"war_pre_mean\")\n",
    "\n",
    "    out = out.merge(pre_sum, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(pre_n, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(pre_mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    # Post: [Y, Y + post-1]\n",
    "    m[\"post_start\"] = m[\"_yr\"]\n",
    "    m[\"post_end\"] = m[\"_yr\"] + (post_years - 1)\n",
    "    post = m[m[\"_season\"].between(m[\"post_start\"], m[\"post_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    post_sum = post.groupby(\"_row_id\")[\"_war\"].sum(min_count=1).rename(\"war_post_sum\")\n",
    "    post_n = post.groupby(\"_row_id\")[\"_season\"].nunique().rename(\"war_post_n\")\n",
    "    post_mean = (post_sum / post_n).rename(\"war_post_mean\")\n",
    "\n",
    "    out = out.merge(post_sum, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(post_n, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(post_mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    for ccol in [\"war_pre_sum\", \"war_pre_mean\", \"war_post_sum\", \"war_post_mean\"]:\n",
    "        out[ccol] = pd.to_numeric(out[ccol], errors=\"coerce\")\n",
    "\n",
    "    # war loss, explanation: + means worse than expected\n",
    "    # mean loss is more easily comparable across post windows\n",
    "    out[\"has_war_pre\"] = out[\"war_pre_n\"].fillna(0).astype(int).clip(lower=0)\n",
    "    out[\"has_war_post\"] = out[\"war_post_n\"].fillna(0).astype(int).clip(lower=0)\n",
    "    out[\"war_loss_mean\"] = (out[\"war_pre_mean\"] - out[\"war_post_mean\"]).clip(lower=0)\n",
    "\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizes the different ISI lambda and lookback combos\n",
    "\n",
    "# function uses regex in case of any convention errors\n",
    "def discover_isi_families(df: pd.DataFrame) -> Dict[Tuple[int, str], List[str]]:\n",
    "    isi_pattern = re.compile(r\"^ISI_lb(\\d+)_lamdba_(\\d+)$\", re.IGNORECASE)\n",
    "    \n",
    "    families = {}\n",
    "    for col in df.columns:\n",
    "        match = isi_pattern.match(col)\n",
    "        if match:\n",
    "            lb = int(match.group(1))\n",
    "            lam = match.group(2)\n",
    "            key = (lb, lam)\n",
    "            \n",
    "            # looks for related cols\n",
    "            suffix = f\"_lb{lb}_lamdba_{lam}\"\n",
    "            related = [c for c in df.columns if suffix in c.lower() or c == col]\n",
    "            families[key] = unique_list(related)\n",
    "    \n",
    "    return families\n",
    "\n",
    "# implementation of new polynomial ISI transformations\n",
    "def add_isi_transforms(df: pd.DataFrame, isi_col: str, prefix: str) -> pd.DataFrame:\n",
    "\n",
    "    if isi_col not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    x = pd.to_numeric(df[isi_col], errors=\"coerce\")\n",
    "    \n",
    "    # transformations\n",
    "    df[f\"{prefix}_isi_raw\"] = x\n",
    "    df[f\"{prefix}_isi_sq\"] = x ** 2\n",
    "    df[f\"{prefix}_isi_sqrt\"] = np.sqrt(np.clip(x, 0, None))\n",
    "    df[f\"{prefix}_isi_log\"] = np.log1p(np.clip(x, 0, None))\n",
    "    \n",
    "    # interactions\n",
    "    if \"years_int\" in df.columns:\n",
    "        yrs = pd.to_numeric(df[\"years_int\"], errors=\"coerce\")\n",
    "        df[f\"{prefix}_isi_x_years\"] = x * yrs\n",
    "    \n",
    "    if \"age_at_signing\" in df.columns:\n",
    "        age = pd.to_numeric(df[\"age_at_signing\"], errors=\"coerce\")\n",
    "        df[f\"{prefix}_isi_x_age\"] = x * age\n",
    "        df[f\"{prefix}_isi_x_age_sq\"] = x * (age ** 2)\n",
    "    \n",
    "    if \"war_pre_mean\" in df.columns:\n",
    "        war = pd.to_numeric(df[\"war_pre_mean\"], errors=\"coerce\")\n",
    "        df[f\"{prefix}_isi_x_war\"] = x * war\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def make_preprocessor(\n",
    "    numeric_features: List[str],\n",
    "    categorical_features: List[str],\n",
    "    use_robust_scaler: bool = False,\n",
    ") -> ColumnTransformer:\n",
    "    # attempts to use Robust Scaler if available\n",
    "    # Robust uses median and IQR rather than normalization\n",
    "    scaler = RobustScaler() if use_robust_scaler else StandardScaler()\n",
    "    \n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", scaler),\n",
    "    ])\n",
    "    \n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ])\n",
    "    \n",
    "    transformers = [(\"num\", num_pipe, numeric_features)]\n",
    "    if categorical_features:\n",
    "        transformers.append((\"cat\", cat_pipe, categorical_features))\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates the model params for uses the Optuna python package\n",
    "# Optuna allows for the model to select params rather than explicitly stating such params\n",
    "\n",
    "def create_model_from_trial(trial: \"optuna.Trial\", model_type: str, random_state: int = 42) -> Any:\n",
    "    \n",
    "    if model_type == \"xgboost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 2000),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 2, 8),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.001, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"xgb_reg_lambda\", 0.1, 10.0, log=True),\n",
    "            \"reg_alpha\": trial.suggest_float(\"xgb_reg_alpha\", 0.001, 10.0, log=True),\n",
    "            \"min_child_weight\": trial.suggest_int(\"xgb_min_child_weight\", 1, 20),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 0.0, 5.0),\n",
    "            \"scale_pos_weight\": trial.suggest_float(\"xgb_scale_pos_weight\", 0.5, 3.0),\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": -1,\n",
    "            \"eval_metric\": \"auc\",\n",
    "            \"use_label_encoder\": False,\n",
    "        }\n",
    "        return XGBClassifier(**params)\n",
    "    \n",
    "    elif model_type == \"random_forest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 3, 20),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"class_weight\": trial.suggest_categorical(\"rf_class_weight\", [\"balanced\", \"balanced_subsample\", None]),\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        return RandomForestClassifier(**params)\n",
    "    \n",
    "    elif model_type == \"gradient_boosting\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"gb_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"gb_max_depth\", 2, 8),\n",
    "            \"learning_rate\": trial.suggest_float(\"gb_learning_rate\", 0.001, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"gb_subsample\", 0.5, 1.0),\n",
    "            \"min_samples_split\": trial.suggest_int(\"gb_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"gb_min_samples_leaf\", 1, 10),\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "        return GradientBoostingClassifier(**params)\n",
    "    \n",
    "    elif model_type == \"hist_gradient_boosting\":\n",
    "        params = {\n",
    "            \"max_iter\": trial.suggest_int(\"hgb_max_iter\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"hgb_max_depth\", 2, 15),\n",
    "            \"learning_rate\": trial.suggest_float(\"hgb_learning_rate\", 0.001, 0.3, log=True),\n",
    "            \"l2_regularization\": trial.suggest_float(\"hgb_l2_reg\", 0.0, 10.0),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"hgb_min_samples_leaf\", 5, 50),\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "        return HistGradientBoostingClassifier(**params)\n",
    "    \n",
    "    elif model_type == \"logistic\":\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"lr_C\", 0.001, 100.0, log=True),\n",
    "            \"l1_ratio\": trial.suggest_float(\"lr_l1_ratio\", 0.0, 1.0),\n",
    "            \"max_iter\": 5000,\n",
    "            \"solver\": \"saga\",\n",
    "            \"penalty\": \"elasticnet\",\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        return LogisticRegression(**params)\n",
    "    \n",
    "    # error is raised if a new model type is added or a typo exists\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fed854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run and eval\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"Computes classification metrics\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "    \n",
    "    mask = np.isfinite(y_prob)\n",
    "    y_true = y_true[mask]\n",
    "    y_prob = y_prob[mask]\n",
    "    \n",
    "    # AUC requires a min of two classes\n",
    "    if len(y_true) == 0 or len(np.unique(y_true)) < 2:\n",
    "        return {\n",
    "            \"AUC\": np.nan, \"ACC\": np.nan, \"F1\": np.nan,\n",
    "            \"LOGLOSS\": np.nan, \"BRIER\": np.nan, \"AP\": np.nan,\n",
    "            \"n_eval\": 0, \"pos_rate\": np.nan,\n",
    "        }\n",
    "    \n",
    "    # grading\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # scores provided by model results\n",
    "    return {\n",
    "        \"AUC\": float(roc_auc_score(y_true, y_prob)),\n",
    "        \"ACC\": float(accuracy_score(y_true, y_hat)),\n",
    "        \"F1\": float(f1_score(y_true, y_hat, zero_division=0)),\n",
    "        \"LOGLOSS\": float(log_loss(y_true, y_prob, labels=[0, 1])),\n",
    "        \"BRIER\": float(brier_score_loss(y_true, y_prob)),\n",
    "        \"AP\": float(average_precision_score(y_true, y_prob)),\n",
    "        \"n_eval\": int(len(y_true)),\n",
    "        \"pos_rate\": float(np.mean(y_true)),\n",
    "    }\n",
    "\n",
    "# this is to determine the threshold for grading which maximizes F1 (0-1 scale)\n",
    "def find_optimal_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, float]:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    \n",
    "    # Compute F1 for each threshold\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # argmax returns the best result\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    return float(best_threshold), float(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ff157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna creation, refer to Optuna documentation on developer website\n",
    "\n",
    "class OptunaObjective:    \n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: np.ndarray,\n",
    "        numeric_features: List[str],\n",
    "        categorical_features: List[str],\n",
    "        n_cv_folds: int = 5,\n",
    "        random_state: int = 42,\n",
    "    ):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_cv_folds = n_cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.cv = StratifiedKFold(n_splits=n_cv_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def __call__(self, trial: \"optuna.Trial\") -> float:\n",
    "        # Select model type\n",
    "        model_type = trial.suggest_categorical(\n",
    "            \"model_type\", \n",
    "            [\"xgboost\", \"random_forest\", \"hist_gradient_boosting\", \"logistic\"]\n",
    "        )\n",
    "        \n",
    "        # Select scaler type\n",
    "        # uses robust as default\n",
    "        # switches to Standard Scaler if Robust Scaler fails\n",
    "        use_robust = trial.suggest_categorical(\"use_robust_scaler\", [True, False])\n",
    "        \n",
    "        # calls functions to run model\n",
    "        model = create_model_from_trial(trial, model_type, self.random_state)\n",
    "        preprocessor = make_preprocessor(\n",
    "            self.numeric_features, \n",
    "            self.categorical_features,\n",
    "            use_robust_scaler=use_robust,\n",
    "        )\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", model),\n",
    "        ])\n",
    "        \n",
    "        # Score CV with AUC\n",
    "        try:\n",
    "            scores = cross_val_score(\n",
    "                pipeline, \n",
    "                self.X_train[self.numeric_features + self.categorical_features],\n",
    "                self.y_train,\n",
    "                cv=self.cv,\n",
    "                scoring=\"roc_auc\",\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            \n",
    "            # returns the mean AUC\n",
    "            return float(np.mean(scores))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Trial failed: {e}\")\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec04591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def load_and_prepare_data(cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame, List[str], List[str]]:\n",
    "    \n",
    "    \n",
    "    # Loads data\n",
    "    df = pd.read_csv(cfg.DATA_PATH)\n",
    "    df = dedupe_columns(df)\n",
    "\n",
    "    bat_rates = pd.read_csv(cfg.BAT_RATES_PATH)\n",
    "    pit_rates = pd.read_csv(cfg.PIT_RATES_PATH)\n",
    "    def_stats = pd.read_csv(cfg.DEF_STATS_PATH)\n",
    "    sc_pit = pd.read_csv(cfg.STATCAST_PIT_PATH)\n",
    "    war_panel = load_war_panel(cfg.WAR_DIR)\n",
    "    \n",
    "    df[\"term_start_year\"] = pd.to_numeric(df.get(\"term_start_year\"), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"term_start_year\"]).copy()\n",
    "    df[\"year\"] = df[\"term_start_year\"].astype(int)\n",
    "    \n",
    "    # Filter term years 2020-2025\n",
    "    df = df[(df[\"year\"] >= 2020) & (df[\"year\"] <= 2025)].copy()\n",
    "    \n",
    "    # Filter contract length 1-5 years\n",
    "    df[\"years_int\"] = pd.to_numeric(df.get(\"years_int\"), errors=\"coerce\")\n",
    "    before = len(df)\n",
    "    df = df[df[\"years_int\"].notna() & (df[\"years_int\"] <= cfg.MAX_CONTRACT_YEARS)].copy()\n",
    "    print(f\"Filtered contracts > {cfg.MAX_CONTRACT_YEARS} years: {before} -> {len(df)}\")\n",
    "    \n",
    "    for c in [\"key_fangraphs\", \"key_mlbam\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    \n",
    "    # apply pitcher flag\n",
    "    df[\"is_pitcher_flag\"] = df[\"position\"].apply(is_pitcher)\n",
    "    \n",
    "    # Apply hitting features\n",
    "    bat_rate_cols = [\n",
    "        \"walk_percent\", \"strikout_percent\", \"AVG\", \"OBP\", \"SLG\", \"OPS\", \n",
    "        \"ISO\", \"wOBA\", \"wRC+\", \"batting_avg_on_balls_in_play\",\n",
    "        \"batted_balls_hard_contact_percent\",\n",
    "    ]\n",
    "    df = add_pre_rate_features(\n",
    "        df, bat_rates, rate_cols=safe_cols(bat_rates, bat_rate_cols),\n",
    "        weight_col=\"PA\" if \"PA\" in bat_rates.columns else None,\n",
    "        prefix=\"bat\", pre_years=cfg.PRE_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Apply pitching features\n",
    "    pit_rate_cols = [\n",
    "        \"strikeout_percent\", \"walk_percent\", \"FIP\", \"xFIP\",\n",
    "        \"batting_average_on_balls_in_play\", \"walks_plus_hits_per_9_innings\",\n",
    "        \"home_runs_allowed_per_9_innings\",\n",
    "    ]\n",
    "    df = add_pre_rate_features(\n",
    "        df, pit_rates, rate_cols=safe_cols(pit_rates, pit_rate_cols),\n",
    "        weight_col=\"TBF\" if \"TBF\" in pit_rates.columns else None,\n",
    "        prefix=\"pit\", pre_years=cfg.PRE_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Apply defensive features\n",
    "    def_feature_cols = [\n",
    "        \"fielding_percentage\", \"defensive_runs_saved\", \"Errors\",\n",
    "    ]\n",
    "    df = add_pre_panel_features(\n",
    "        df, def_stats, contract_key_col=\"key_mlbam\", panel_key_col=\"MLBAMID\",\n",
    "        contract_year_col=\"year\", panel_year_col=\"year\",\n",
    "        feature_cols=safe_cols(def_stats, def_feature_cols),\n",
    "        weight_col=\"Innings_played\" if \"Innings_played\" in def_stats.columns else None,\n",
    "        prefix=\"def\", pre_years=cfg.PRE_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Apply statcast pitching features\n",
    "    sc_feature_cols = [\n",
    "        \"fastball_avg_speed\", \"whiff_percent\", \"hard_hit_percent\",\n",
    "        \"barrel_batted_rate\", \"exit_velocity_avg\", \"swing_percent\",\n",
    "    ]\n",
    "    df = add_pre_panel_features(\n",
    "        df, sc_pit, contract_key_col=\"key_mlbam\", panel_key_col=\"player_id\",\n",
    "        contract_year_col=\"year\", panel_year_col=\"year\",\n",
    "        feature_cols=safe_cols(sc_pit, sc_feature_cols),\n",
    "        weight_col=\"pa\" if \"pa\" in sc_pit.columns else None,\n",
    "        prefix=\"scpit\", pre_years=cfg.PRE_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Apply WAR stats\n",
    "    df = add_war_windows(\n",
    "        df, war_panel, key_col=\"key_fangraphs\", year_col=\"year\",\n",
    "        pre_years=cfg.PRE_YEARS, post_years=cfg.PRIMARY_POST_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Filters df for rows with WAR data only\n",
    "    before = len(df)\n",
    "    df = df[(df[\"has_war_pre\"] > 0) & (df[\"has_war_post\"] > 0)].copy()\n",
    "    print(f\"Filtered missing WAR: {before} -> {len(df)}\")\n",
    "    \n",
    "    # Finds ISI combos (lambda and lookback periods)\n",
    "    # apply transformations to each combo\n",
    "    isi_families = discover_isi_families(df)\n",
    "    for (lb, lam), cols in isi_families.items():\n",
    "        isi_main_col = f\"ISI_lb{lb}_lamdba_{lam}\"\n",
    "        if isi_main_col in df.columns:\n",
    "            prefix = f\"isi_{lb}_{lam}\"\n",
    "            df = add_isi_transforms(df, isi_main_col, prefix)\n",
    "    \n",
    "    df = dedupe_columns(df)\n",
    "    \n",
    "    BASE_NUMERIC = [\n",
    "        \"age_at_signing\", \"years_int\", \"opt_out_flag\", \"year\", \"is_pitcher_flag\",\n",
    "        \"war_pre_mean\", \"war_pre_sum\",\n",
    "    ]\n",
    "    BASE_CATEGORICAL = [\"position\"]\n",
    "    \n",
    "    # detect newly created features\n",
    "    PREFIXES = (\"bat_pre_\", \"pit_pre_\", \"def_pre_\", \"scpit_pre_\", \"isi_\")\n",
    "    generated_feats = [c for c in df.columns if c.startswith(PREFIXES)]\n",
    "    coverage_feats = [c for c in df.columns if c.startswith(\"has_\") or c.endswith(\"_seasons\")]\n",
    "    \n",
    "    # includes raw ISI columns\n",
    "    isi_raw_cols = [c for c in df.columns if c.startswith(\"ISI_\") or \"surgery\" in c.lower() or \"structural\" in c.lower()]\n",
    "    \n",
    "    numeric_features = unique_list([\n",
    "        c for c in (BASE_NUMERIC + generated_feats + coverage_feats + isi_raw_cols)\n",
    "        if c in df.columns\n",
    "    ])\n",
    "    categorical_features = unique_list([\n",
    "        c for c in BASE_CATEGORICAL if c in df.columns\n",
    "    ])\n",
    "    \n",
    "    print(f\"Total numeric features: {len(numeric_features)}\")\n",
    "    print(f\"Total categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    return df, war_panel, numeric_features, categorical_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Loop\n",
    "def run_optimization(cfg: Config):\n",
    "    # time limit    \n",
    "    start_time = time.time()\n",
    "    max_runtime_seconds = cfg.MAX_RUNTIME_HOURS * 3600\n",
    "    \n",
    "    # creates output dir (EDIT IN CONFIG)\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    df, war_panel, numeric_features, categorical_features = load_and_prepare_data(cfg)\n",
    "    \n",
    "    # define target\n",
    "    threshold = cfg.PRIMARY_WAR_THRESHOLD\n",
    "    df[\"target\"] = (df[\"war_loss_mean\"] >= threshold).astype(int)\n",
    "    \n",
    "    print(f\"\\nTarget threshold: WAR loss >= {threshold}\")\n",
    "    print(f\"Positive rate: {df['target'].mean():.3f}\")\n",
    "    \n",
    "    train_df, test_df = time_split(df, cfg.TRAIN_YEARS, cfg.TEST_YEARS)\n",
    "    \n",
    "    # applies percentile cutoff\n",
    "    aav_col = \"guarantee_real_per_year_2025\"\n",
    "    if aav_col in train_df.columns:\n",
    "        cutoff = train_df[aav_col].quantile(0.95)\n",
    "        train_df = train_df[train_df[aav_col] <= cutoff].copy()\n",
    "        test_df = test_df[test_df[aav_col] <= cutoff].copy()\n",
    "        print(f\"Dataset size with Top 5% cutoff applied: Train={len(train_df)} | Test={len(test_df)}\")\n",
    "    \n",
    "    feature_cols = numeric_features + categorical_features\n",
    "    X_train = train_df[feature_cols].copy()\n",
    "    y_train = train_df[\"target\"].values\n",
    "    X_test = test_df[feature_cols].copy()\n",
    "    y_test = test_df[\"target\"].values\n",
    "    \n",
    "    # saves results\n",
    "    all_results = []\n",
    "    best_auc = 0.0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    last_improvement_time = time.time()\n",
    "    \n",
    "    # runs optuna optimization parameters\n",
    "    if OPTUNA_AVAILABLE:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            sampler=TPESampler(seed=cfg.RANDOM_STATE),\n",
    "            pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5),\n",
    "        )\n",
    "        \n",
    "        # defines goal\n",
    "        objective = OptunaObjective(\n",
    "            X_train, y_train, numeric_features, categorical_features,\n",
    "            n_cv_folds=cfg.N_CV_FOLDS, random_state=cfg.RANDOM_STATE,\n",
    "        )\n",
    "        \n",
    "        # callback for early stopping params and time limit\n",
    "        def callback(study, trial):\n",
    "            nonlocal best_auc, last_improvement_time\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if elapsed > max_runtime_seconds:\n",
    "                print(f\"\\n[TIME LIMIT] Reached {cfg.MAX_RUNTIME_HOURS} hours\")\n",
    "                study.stop()\n",
    "                return\n",
    "            \n",
    "            # compares current and best results\n",
    "            if trial.value and trial.value > best_auc:\n",
    "                best_auc = trial.value\n",
    "                last_improvement_time = time.time()\n",
    "                print(f\"[NEW BEST] Trial {trial.number}: AUC={trial.value:.4f}\")\n",
    "            \n",
    "            # Stops model if no improvements are being made\n",
    "            no_improve_seconds = time.time() - last_improvement_time\n",
    "            if no_improve_seconds > cfg.EARLY_STOP_NO_IMPROVE_MINUTES * 60:\n",
    "                print(f\"\\n[EARLY STOP] No improvement for {cfg.EARLY_STOP_NO_IMPROVE_MINUTES} minutes\")\n",
    "                study.stop()\n",
    "                return\n",
    "            \n",
    "            # Progress update every 10 trials\n",
    "            if trial.number % 10 == 0:\n",
    "                remaining = (max_runtime_seconds - elapsed) / 60\n",
    "                print(f\"[PROGRESS] Trial {trial.number} | Best AUC: {study.best_value:.4f} | Time remaining: {remaining:.1f} min\")\n",
    "        \n",
    "        # run\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=cfg.N_OPTUNA_TRIALS,\n",
    "            callbacks=[callback],\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\Optimization completed: {len(study.trials)} trials\")\n",
    "        print(f\"Best CV AUC: {study.best_value:.4f}\")\n",
    "        print(f\"Best params: {study.best_params}\")\n",
    "        \n",
    "        # Train final model with best params\n",
    "        print(\"\\n\" + \"=\" * 20)\n",
    "        print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "        print(\"=\" * 20)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        model_type = best_params[\"model_type\"]\n",
    "        \n",
    "        # recreates the best model\n",
    "        class FinalTrial:\n",
    "            def __init__(self, params):\n",
    "                self.params = params\n",
    "            def suggest_categorical(self, name, choices):\n",
    "                return self.params.get(name, choices[0])\n",
    "            def suggest_int(self, name, low, high):\n",
    "                return self.params.get(name, (low + high) // 2)\n",
    "            def suggest_float(self, name, low, high, log=False):\n",
    "                return self.params.get(name, (low + high) / 2)\n",
    "        \n",
    "        final_trial = FinalTrial(best_params)\n",
    "        best_model = create_model_from_trial(final_trial, model_type, cfg.RANDOM_STATE)\n",
    "        use_robust = best_params.get(\"use_robust_scaler\", False)\n",
    "        \n",
    "        preprocessor = make_preprocessor(numeric_features, categorical_features, use_robust)\n",
    "        \n",
    "        final_pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", best_model),\n",
    "        ])\n",
    "        \n",
    "        final_pipeline.fit(X_train, y_train)\n",
    "        y_prob_test = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "        test_metrics = compute_metrics(y_test, y_prob_test)\n",
    "        \n",
    "        # define optimal threshold\n",
    "        opt_threshold, opt_f1 = find_optimal_threshold(y_test, y_prob_test)\n",
    "        test_metrics_opt = compute_metrics(y_test, y_prob_test, threshold=opt_threshold)\n",
    "        \n",
    "        print(f\"\\nTest Set Results (threshold=0.5):\")\n",
    "        print(f\"  AUC: {test_metrics['AUC']:.4f}\")\n",
    "        print(f\"  Accuracy: {test_metrics['ACC']:.4f}\")\n",
    "        print(f\"  F1: {test_metrics['F1']:.4f}\")\n",
    "        print(f\"  Brier: {test_metrics['BRIER']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nTest Set Results (optimal threshold={opt_threshold:.3f}):\")\n",
    "        print(f\"  F1: {test_metrics_opt['F1']:.4f}\")\n",
    "        \n",
    "        # save results\n",
    "        result_row = {\n",
    "            \"model_type\": model_type,\n",
    "            \"cv_auc\": study.best_value,\n",
    "            \"test_auc\": test_metrics[\"AUC\"],\n",
    "            \"test_acc\": test_metrics[\"ACC\"],\n",
    "            \"test_f1_default\": test_metrics[\"F1\"],\n",
    "            \"test_f1_optimal\": test_metrics_opt[\"F1\"],\n",
    "            \"optimal_threshold\": opt_threshold,\n",
    "            \"test_brier\": test_metrics[\"BRIER\"],\n",
    "            \"test_logloss\": test_metrics[\"LOGLOSS\"],\n",
    "            \"n_train\": len(train_df),\n",
    "            \"n_test\": len(test_df),\n",
    "            \"pos_rate_train\": y_train.mean(),\n",
    "            \"pos_rate_test\": y_test.mean(),\n",
    "            \"war_threshold\": threshold,\n",
    "            \"n_trials\": len(study.trials),\n",
    "            **{f\"param_{k}\": v for k, v in best_params.items()},\n",
    "        }\n",
    "        all_results.append(result_row)\n",
    "        \n",
    "        # records best trials\n",
    "        top_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:cfg.TOP_N_MODELS]\n",
    "        \n",
    "        trial_results = []\n",
    "        for t in top_trials:\n",
    "            if t.value:\n",
    "                trial_results.append({\"trial_number\": t.number, \"cv_auc\": t.value, **t.params,})\n",
    "        \n",
    "        trials_df = pd.DataFrame(trial_results)\n",
    "        trials_path = os.path.join(cfg.OUTPUT_DIR, \"optuna_top_trials.csv\")\n",
    "        trials_df.to_csv(trials_path, index=False)\n",
    "        print(f\"\\nSaved top {len(trial_results)} trials to: {trials_path}\")\n",
    "        \n",
    "        # saves the trained model\n",
    "        model_path = os.path.join(cfg.OUTPUT_DIR, \"best_model.pkl\")\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(final_pipeline, f)\n",
    "        print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "    # BACKUP IN CASE OPTUNA FAILS  \n",
    "    else:        \n",
    "        models_to_try = [\n",
    "            (\"XGB_default\", XGBClassifier(\n",
    "                n_estimators=1000, max_depth=4, learning_rate=0.05,\n",
    "                subsample=0.8, colsample_bytree=0.8, reg_lambda=1.5,\n",
    "                random_state=cfg.RANDOM_STATE, n_jobs=-1,\n",
    "            )),\n",
    "            (\"RF_default\", RandomForestClassifier(\n",
    "                n_estimators=500, max_depth=10, class_weight=\"balanced\",\n",
    "                random_state=cfg.RANDOM_STATE, n_jobs=-1,\n",
    "            )),\n",
    "            (\"HGB_default\", HistGradientBoostingClassifier(\n",
    "                max_iter=500, max_depth=6, learning_rate=0.05,\n",
    "                random_state=cfg.RANDOM_STATE,\n",
    "            )),\n",
    "        ]\n",
    "        preprocessor = make_preprocessor(numeric_features, categorical_features, False)\n",
    "\n",
    "        for model_name, model in models_to_try:            \n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"classifier\", model),\n",
    "            ])\n",
    "            \n",
    "            # CV eval\n",
    "            cv = StratifiedKFold(n_splits=cfg.N_CV_FOLDS, shuffle=True, random_state=cfg.RANDOM_STATE)\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_prob_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "            test_metrics = compute_metrics(y_test, y_prob_test)\n",
    "            \n",
    "            print(f\"  CV AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "            print(f\"  Test AUC: {test_metrics['AUC']:.4f}\")\n",
    "            all_results.append({\n",
    "                \"model_type\": model_name,\n",
    "                \"cv_auc\": np.mean(cv_scores),\n",
    "                \"cv_auc_std\": np.std(cv_scores),\n",
    "                **test_metrics,\n",
    "            })\n",
    "    \n",
    "    # save final results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_path = os.path.join(cfg.OUTPUT_DIR, \"optimization_results.csv\")\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nSaved results to: {results_path}\")\n",
    "    \n",
    "    # print results summary \n",
    "    if len(all_results) > 0:\n",
    "        best = max(all_results, key=lambda x: x.get(\"test_auc\", 0))\n",
    "        print(f\"\\nBest Model: {best.get('model_type', 'N/A')}\")\n",
    "        print(f\"Test AUC: {best.get('test_auc', 'N/A'):.4f}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main run\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Max runtime: {Config.MAX_RUNTIME_HOURS} hours\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    # init config\n",
    "    cfg = Config()\n",
    "    \n",
    "    # confirm data paths\n",
    "    missing_paths = []\n",
    "    for path_name in [\"DATA_PATH\", \"BAT_RATES_PATH\", \"PIT_RATES_PATH\", \"DEF_STATS_PATH\", \"STATCAST_PIT_PATH\", \"WAR_DIR\"]:\n",
    "        path = getattr(cfg, path_name)\n",
    "        if not os.path.exists(path):\n",
    "            missing_paths.append(f\"{path_name}: {path}\")\n",
    "    \n",
    "    if missing_paths:\n",
    "        print(\"\\n[ERROR] Missing paths:\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # optim run\n",
    "    try:\n",
    "        results = run_optimization(cfg)\n",
    "                \n",
    "        print(\"\\n\" + \"=\" * 20)\n",
    "        print(\"TRAINING COMPLETE\")\n",
    "        print(\"=\" * 20)\n",
    "        print(f\"Output directory: {cfg.OUTPUT_DIR}\")\n",
    "        print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Optimization failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabr_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
