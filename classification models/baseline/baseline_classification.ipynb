{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports \n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, SplineTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14116a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "DATA_PATH = r\"contracts_with_isi_v2_SWEEP_WIDE_WITH_KEYS_PLUS_CPI.csv\"\n",
    "\n",
    "BAT_RATES_PATH = r\"batting_rates_by_season.csv\"\n",
    "PIT_RATES_PATH = r\"pitching_rates_by_season.csv\"\n",
    "DEF_STATS_PATH = r\"defensive_stats.csv\"\n",
    "STATCAST_PIT_PATH = r\"statcast_pitching_2015_2025.csv\"\n",
    "\n",
    "# WAR folder directory (contains war files from 2010-2025)\n",
    "WAR_DIR = r\"war_rankings_raw\\war\"\n",
    "\n",
    "TRAIN_YEARS = {2020, 2021, 2022, 2023}\n",
    "TEST_YEARS  = {2024, 2025}\n",
    "\n",
    "\n",
    "PRE_YEARS = 3\n",
    "POST_YEARS = 3\n",
    "\n",
    "MAX_YEARS = 5\n",
    "\n",
    "AAV_COL = \"guarantee_real_per_year_2025\"\n",
    "REMOVE_TOP_PCTL = 0.95\n",
    "\n",
    "# Threshold sweep\n",
    "WAR_LOSS_THRESHOLDS = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "\n",
    "# Output\n",
    "OUT_CLS_RESULTS = r\"model_comparison_classification_BASELINE_threshold_sweep.csv\"\n",
    "\n",
    "BASE_NUMERIC = [\n",
    "    \"age_at_signing\",\n",
    "    \"years_int\",\n",
    "    \"opt_out_flag\",\n",
    "    \"year\",\n",
    "    \"is_pitcher_flag\",\n",
    "\n",
    "    \"war_pre_mean\",\n",
    "    \"war_pre_sum\",\n",
    "    \"war_pre_n\",\n",
    "]\n",
    "\n",
    "BASE_CATEGORICAL = [\n",
    "    \"position\",\n",
    "    \"qualifying_offer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# drop dups, preserves order\n",
    "def unique_list(seq):\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "# drop dup col names\n",
    "def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "\n",
    "PITCHER_PREFIXES = (\"P\", \"SP\", \"RP\", \"RHP\", \"LHP\")\n",
    "def is_pitcher(pos) -> int:\n",
    "    if pd.isna(pos):\n",
    "        return 0\n",
    "    s = str(pos).strip().upper()\n",
    "    # handles pitcher positional variations\n",
    "    return int(s.startswith(PITCHER_PREFIXES) or (\"RHP\" in s) or (\"LHP\" in s))\n",
    "\n",
    "def time_split(df: pd.DataFrame, year_col: str = \"year\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    y = pd.to_numeric(df[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    train = df[y.isin(TRAIN_YEARS)].copy()\n",
    "    test  = df[y.isin(TEST_YEARS)].copy()\n",
    "    return train, test\n",
    "\n",
    "def classification_metrics(y_true: np.ndarray, y_prob: np.ndarray, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob).astype(float)\n",
    "\n",
    "    mask = np.isfinite(y_prob) & np.isfinite(y_true)\n",
    "    y_true = y_true[mask]\n",
    "    y_prob = y_prob[mask]\n",
    "\n",
    "    # Data integrity check\n",
    "    # # AUC undefined if only one class present\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        auc = np.nan\n",
    "    else:\n",
    "        auc = float(roc_auc_score(y_true, y_prob))\n",
    "\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    f1  = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1\": f1,\n",
    "        \"n_eval\": int(len(y_true)),\n",
    "        \"pos_rate_eval\": float(y_true.mean()) if len(y_true) else np.nan,\n",
    "    }\n",
    "\n",
    "def _safe_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def _weighted_mean(series: pd.Series, weights: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    w = pd.to_numeric(weights, errors=\"coerce\").fillna(0.0)\n",
    "    mask = np.isfinite(s) & np.isfinite(w) & (w > 0)\n",
    "    if mask.sum() == 0:\n",
    "        s2 = s[np.isfinite(s)]\n",
    "        return float(s2.mean()) if len(s2) else np.nan\n",
    "    return float(np.average(s[mask], weights=w[mask]))\n",
    "\n",
    "def add_pre_rate_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    season_rates: pd.DataFrame,\n",
    "    *,\n",
    "    rate_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"key_fangraphs\"] = pd.to_numeric(dfc[\"key_fangraphs\"], errors=\"coerce\")\n",
    "    dfc[\"year\"] = pd.to_numeric(dfc[\"year\"], errors=\"coerce\")\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "\n",
    "    dfs = season_rates.copy()\n",
    "    dfs[\"playerId\"] = pd.to_numeric(dfs[\"playerId\"], errors=\"coerce\")\n",
    "    dfs[\"Season\"] = pd.to_numeric(dfs[\"Season\"], errors=\"coerce\")\n",
    "\n",
    "    dfs = _safe_numeric(dfs, rate_cols + ([weight_col] if weight_col else []))\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"key_fangraphs\", \"year\"]].merge(\n",
    "        dfs,\n",
    "        left_on=\"key_fangraphs\",\n",
    "        right_on=\"playerId\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    m[\"lb_start\"] = m[\"year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"year\"] - 1\n",
    "    m = m[m[\"Season\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"Season\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        rel_sum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_reliability_sum\")\n",
    "        out = out.merge(rel_sum, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "        out[f\"{prefix}_pre_reliability_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_reliability_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    for rc in rate_cols:\n",
    "        feat_name = f\"{prefix}_pre_{rc}\"\n",
    "        if rc not in m.columns:\n",
    "            out[feat_name] = np.nan\n",
    "            continue\n",
    "\n",
    "        if weight_col and weight_col in m.columns:\n",
    "            agg = m.groupby(\"_row_id\").apply(lambda g: _weighted_mean(g[rc], g[weight_col])).rename(feat_name)\n",
    "        else:\n",
    "            agg = m.groupby(\"_row_id\")[rc].mean().rename(feat_name)\n",
    "\n",
    "        out = out.merge(agg, left_on=\"_row_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre panel features\n",
    "\n",
    "def add_pre_panel_features(\n",
    "    contracts: pd.DataFrame,\n",
    "    panel: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str,\n",
    "    panel_key_col: str,\n",
    "    contract_year_col: str,\n",
    "    panel_year_col: str,\n",
    "    feature_cols: List[str],\n",
    "    weight_col: str | None,\n",
    "    prefix: str,\n",
    "    pre_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Generic season panel aggregator (only used for defense and statcast files) \"\"\"\n",
    "    # Data integrity\n",
    "    # Missing value check\n",
    "    missing_contract = [c for c in [contract_key_col, contract_year_col] if c not in contracts.columns]\n",
    "    missing_panel = [c for c in [panel_key_col, panel_year_col] if c not in panel.columns]\n",
    "    if missing_contract:\n",
    "        raise KeyError(f\"[add_pre_panel_features] contracts missing: {missing_contract}\")\n",
    "    if missing_panel:\n",
    "        raise KeyError(f\"[add_pre_panel_features] panel missing: {missing_panel}\")\n",
    "\n",
    "    out = contracts.copy()\n",
    "    out[\"_row_id\"] = np.arange(len(out), dtype=int)\n",
    "\n",
    "    # temp cols\n",
    "    dfc = out[[\"_row_id\", contract_key_col, contract_year_col]].copy()\n",
    "    dfc[\"_contract_key\"] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[\"_contract_year\"] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "\n",
    "    # returns necessary cols only\n",
    "    keep_feats = [c for c in feature_cols if c in panel.columns]\n",
    "    keep_cols = [panel_key_col, panel_year_col] + keep_feats\n",
    "    if weight_col and weight_col in panel.columns and weight_col not in keep_cols:\n",
    "        keep_cols.append(weight_col)\n",
    "\n",
    "    p = panel[keep_cols].copy()\n",
    "    p[\"_panel_key\"] = pd.to_numeric(p[panel_key_col], errors=\"coerce\")\n",
    "    p[\"_panel_year\"] = pd.to_numeric(p[panel_year_col], errors=\"coerce\")\n",
    "\n",
    "    for c in keep_feats:\n",
    "        p[c] = pd.to_numeric(p[c], errors=\"coerce\")\n",
    "    if weight_col and weight_col in p.columns:\n",
    "        p[weight_col] = pd.to_numeric(p[weight_col], errors=\"coerce\")\n",
    "\n",
    "    merge_cols = [\"_panel_key\", \"_panel_year\"] + keep_feats + ([weight_col] if (weight_col and weight_col in p.columns) else [])\n",
    "    m = dfc[[\"_row_id\", \"_contract_key\", \"_contract_year\"]].merge(\n",
    "        p[merge_cols],\n",
    "        left_on=\"_contract_key\",\n",
    "        right_on=\"_panel_key\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Apply lookback filter\n",
    "    m[\"lb_start\"] = m[\"_contract_year\"] - pre_years\n",
    "    m[\"lb_end\"] = m[\"_contract_year\"] - 1\n",
    "    m = m[m[\"_panel_year\"].between(m[\"lb_start\"], m[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    cov = m.groupby(\"_row_id\")[\"_panel_year\"].nunique().rename(f\"{prefix}_pre_seasons\")\n",
    "    out = out.merge(cov, on=\"_row_id\", how=\"left\")\n",
    "    out[f\"{prefix}_pre_seasons\"] = out[f\"{prefix}_pre_seasons\"].fillna(0).astype(int)\n",
    "\n",
    "    # Weighted sums\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        wsum = m.groupby(\"_row_id\")[weight_col].sum(min_count=1).rename(f\"{prefix}_pre_weight_sum\")\n",
    "        out = out.merge(wsum, on=\"_row_id\", how=\"left\")\n",
    "        out[f\"{prefix}_pre_weight_sum\"] = pd.to_numeric(out[f\"{prefix}_pre_weight_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Aggregate features where necessary\n",
    "    if weight_col and weight_col in m.columns:\n",
    "        w = pd.to_numeric(m[weight_col], errors=\"coerce\").fillna(0.0)\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            x = pd.to_numeric(m[fc], errors=\"coerce\")\n",
    "            num = (x * w).groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            den = w.groupby(m[\"_row_id\"]).sum(min_count=1)\n",
    "            wmean = (num / den).replace([np.inf, -np.inf], np.nan).rename(feat_name)\n",
    "            out = out.merge(wmean, on=\"_row_id\", how=\"left\")\n",
    "    else:\n",
    "        for fc in keep_feats:\n",
    "            feat_name = f\"{prefix}_pre_{fc}\"\n",
    "            mean = m.groupby(\"_row_id\")[fc].mean().rename(feat_name)\n",
    "            out = out.merge(mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    out[f\"has_{prefix}_pre\"] = (out[f\"{prefix}_pre_seasons\"] > 0).astype(int)\n",
    "    out = out.drop(columns=[\"_row_id\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd993d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Utilize WAR Statistics\n",
    "\n",
    "# Data loader & Data Integrity Check\n",
    "def load_war_files(war_dir: str) -> pd.DataFrame:\n",
    "    paths = sorted(glob.glob(os.path.join(war_dir, \"war_*.csv\")))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No war_*.csv files found in {war_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p)\n",
    "        df = dedupe_columns(df)\n",
    "        if \"PlayerId\" not in df.columns and \"playerid\" in [c.lower() for c in df.columns]:\n",
    "            # Fallback for missing values\n",
    "            for c in df.columns:\n",
    "                if c.lower() == \"playerid\":\n",
    "                    df = df.rename(columns={c: \"PlayerId\"})\n",
    "        if \"Season\" not in df.columns and \"season\" in [c.lower() for c in df.columns]:\n",
    "            for c in df.columns:\n",
    "                if c.lower() == \"season\":\n",
    "                    df = df.rename(columns={c: \"Season\"})\n",
    "\n",
    "        # Ensures no naming conventions change in future years\n",
    "        # Total WAR column may vary; handles common variants\n",
    "        war_col = None\n",
    "        for c in df.columns:\n",
    "            if c.strip().lower() in {\"total war\", \"total_war\", \"war\", \"totalwar\"}:\n",
    "                war_col = c\n",
    "                break\n",
    "        if war_col is None:\n",
    "            raise KeyError(f\"Could not find Total WAR column in {os.path.basename(p)}. Columns: {list(df.columns)}\")\n",
    "\n",
    "        df = df.rename(columns={war_col: \"TotalWAR\"}).copy()\n",
    "        df[\"PlayerId\"] = pd.to_numeric(df[\"PlayerId\"], errors=\"coerce\")\n",
    "        df[\"Season\"] = pd.to_numeric(df[\"Season\"], errors=\"coerce\")\n",
    "        df[\"TotalWAR\"] = pd.to_numeric(df[\"TotalWAR\"], errors=\"coerce\")\n",
    "\n",
    "        dfs.append(df[[\"PlayerId\", \"Season\", \"TotalWAR\"]])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    out = out.dropna(subset=[\"PlayerId\", \"Season\"]).copy()\n",
    "    out[\"PlayerId\"] = out[\"PlayerId\"].astype(int)\n",
    "    out[\"Season\"] = out[\"Season\"].astype(int)\n",
    "    return out\n",
    "\n",
    "def add_war_pre_post(\n",
    "    contracts: pd.DataFrame,\n",
    "    war: pd.DataFrame,\n",
    "    *,\n",
    "    contract_key_col: str = \"key_fangraphs\",\n",
    "    contract_year_col: str = \"year\",\n",
    "    pre_years: int = 3,\n",
    "    post_years: int = 3,\n",
    ") -> pd.DataFrame:\n",
    "    dfc = contracts.copy()\n",
    "    dfc[\"_row_id\"] = np.arange(len(dfc), dtype=int)\n",
    "    dfc[\"_pid\"] = pd.to_numeric(dfc[contract_key_col], errors=\"coerce\")\n",
    "    dfc[\"_yr\"] = pd.to_numeric(dfc[contract_year_col], errors=\"coerce\")\n",
    "\n",
    "    w = war.copy()\n",
    "    w[\"_pid\"] = pd.to_numeric(w[\"PlayerId\"], errors=\"coerce\")\n",
    "    w[\"_season\"] = pd.to_numeric(w[\"Season\"], errors=\"coerce\")\n",
    "    w[\"_war\"] = pd.to_numeric(w[\"TotalWAR\"], errors=\"coerce\")\n",
    "\n",
    "    m = dfc[[\"_row_id\", \"_pid\", \"_yr\"]].merge(\n",
    "        w[[\"_pid\", \"_season\", \"_war\"]],\n",
    "        on=\"_pid\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    out = dfc.copy()\n",
    "\n",
    "    # pre_war: [Y-pre, Y-1]\n",
    "    m_pre = m.copy()\n",
    "    m_pre[\"lb_start\"] = m_pre[\"_yr\"] - pre_years\n",
    "    m_pre[\"lb_end\"] = m_pre[\"_yr\"] - 1\n",
    "    m_pre = m_pre[m_pre[\"_season\"].between(m_pre[\"lb_start\"], m_pre[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    pre_n = m_pre.groupby(\"_row_id\")[\"_season\"].nunique().rename(\"war_pre_n\")\n",
    "    pre_sum = m_pre.groupby(\"_row_id\")[\"_war\"].sum(min_count=1).rename(\"war_pre_sum\")\n",
    "    pre_mean = m_pre.groupby(\"_row_id\")[\"_war\"].mean().rename(\"war_pre_mean\")\n",
    "\n",
    "    out = out.merge(pre_n, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(pre_sum, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(pre_mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    # post_war: [Y, Y + post-1]\n",
    "    m_post = m.copy()\n",
    "    m_post[\"lb_start\"] = m_post[\"_yr\"]\n",
    "    m_post[\"lb_end\"] = m_post[\"_yr\"] + (post_years - 1)\n",
    "    m_post = m_post[m_post[\"_season\"].between(m_post[\"lb_start\"], m_post[\"lb_end\"], inclusive=\"both\")].copy()\n",
    "\n",
    "    post_n = m_post.groupby(\"_row_id\")[\"_season\"].nunique().rename(\"war_post_n\")\n",
    "    post_sum = m_post.groupby(\"_row_id\")[\"_war\"].sum(min_count=1).rename(\"war_post_sum\")\n",
    "    post_mean = m_post.groupby(\"_row_id\")[\"_war\"].mean().rename(\"war_post_mean\")\n",
    "\n",
    "    out = out.merge(post_n, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(post_sum, on=\"_row_id\", how=\"left\")\n",
    "    out = out.merge(post_mean, on=\"_row_id\", how=\"left\")\n",
    "\n",
    "    # war loss, explanation: + means worse than expected\n",
    "    # mean loss is more easily comparable across post windows\n",
    "    out[\"war_loss_mean\"] = pd.to_numeric(out[\"war_pre_mean\"], errors=\"coerce\") - pd.to_numeric(out[\"war_post_mean\"], errors=\"coerce\")\n",
    "    out[\"war_loss_sum\"]  = pd.to_numeric(out[\"war_pre_sum\"], errors=\"coerce\")  - pd.to_numeric(out[\"war_post_sum\"], errors=\"coerce\")\n",
    "\n",
    "    # cleanup\n",
    "    out = out.drop(columns=[\"_row_id\", \"_pid\", \"_yr\"], errors=\"ignore\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def make_preprocessor(\n",
    "    numeric_features: List[str],\n",
    "    categorical_features: List[str],\n",
    "    use_splines: bool,\n",
    ") -> ColumnTransformer:\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    if use_splines:\n",
    "        num_steps += [\n",
    "            (\"splines\", SplineTransformer(n_knots=6, degree=3, include_bias=False)),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        ]\n",
    "    else:\n",
    "        num_steps += [(\"scaler\", StandardScaler())]\n",
    "\n",
    "    num_pipe = Pipeline(steps=num_steps)\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_features),\n",
    "            (\"cat\", cat_pipe, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "# Using LogL2,LogEN, RF, GAMSL, XGB Sweep\n",
    "\n",
    "def get_classification_models(random_state: int = 42) -> Dict[str, object]:\n",
    "    models: Dict[str, object] = {\n",
    "        \"Logistic_L2\": LogisticRegression(max_iter=50000, solver=\"lbfgs\"),\n",
    "        \"Logistic_ElasticNet\": LogisticRegression(max_iter=50000, solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.35),\n",
    "\n",
    "        \"RandomForest_v2\": RandomForestClassifier(\n",
    "            n_estimators=1500,\n",
    "            max_depth=None,\n",
    "            min_samples_split=6,\n",
    "            min_samples_leaf=3,\n",
    "            max_features=\"sqrt\",\n",
    "            bootstrap=True,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "\n",
    "        \"GAM_Splines_Logit\": LogisticRegression(max_iter=50000, solver=\"lbfgs\"),\n",
    "    }\n",
    "\n",
    "    # XGB sweep config\n",
    "    xgb_grid = [\n",
    "        {\"n_estimators\": 600,  \"learning_rate\": 0.05,  \"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 900,  \"learning_rate\": 0.04,  \"max_depth\": 3, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 1200, \"learning_rate\": 0.03,  \"max_depth\": 4, \"subsample\": 0.85, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.0},\n",
    "        {\"n_estimators\": 1400, \"learning_rate\": 0.025, \"max_depth\": 4, \"subsample\": 0.80, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.5},\n",
    "        {\"n_estimators\": 1600, \"learning_rate\": 0.02,  \"max_depth\": 5, \"subsample\": 0.80, \"colsample_bytree\": 0.80, \"reg_lambda\": 2.0},\n",
    "        {\"n_estimators\": 2500, \"learning_rate\": 0.005, \"max_depth\": 4, \"subsample\": 0.80, \"colsample_bytree\": 0.85, \"reg_lambda\": 1.5},\n",
    "    ]\n",
    "\n",
    "    # XGB Setup (applied once, rather than for each sweep iteration)\n",
    "    xgb_defaults = dict(\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\",\n",
    "        min_child_weight=6,\n",
    "        reg_alpha=0.0,\n",
    "    )\n",
    "\n",
    "    # assigns a name for each XGB sweep model\n",
    "    # ensures no overwriting of previous iterations\n",
    "    for i, params in enumerate(xgb_grid, start=1):\n",
    "        name = f\"XGB_{i:02d}_ne{params['n_estimators']}_lr{params['learning_rate']}_md{params['max_depth']}\"\n",
    "        models[name] = XGBClassifier(**xgb_defaults, **params)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# Model training\n",
    "def fit_predict_classification(\n",
    "    model_name: str,\n",
    "    model,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    *,\n",
    "    numeric_features: List[str],\n",
    "    categorical_features: List[str],\n",
    ") -> np.ndarray:\n",
    "    use_splines = (model_name == \"GAM_Splines_Logit\")\n",
    "    pre = make_preprocessor(numeric_features, categorical_features, use_splines=use_splines)\n",
    "\n",
    "    clf = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # probability of class 1\n",
    "    prob = clf.predict_proba(X_test)[:, 1]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runs baseline classification models that are set up above\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # data loader\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "    bat_rates = pd.read_csv(BAT_RATES_PATH)\n",
    "    pit_rates = pd.read_csv(PIT_RATES_PATH)\n",
    "    def_stats = pd.read_csv(DEF_STATS_PATH)\n",
    "    sc_pit = pd.read_csv(STATCAST_PIT_PATH)\n",
    "    war = load_war_files(WAR_DIR)\n",
    "\n",
    "    # Data integrity\n",
    "    df = dedupe_columns(df)\n",
    "\n",
    "    df[\"term_start_year\"] = pd.to_numeric(df.get(\"term_start_year\"), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"term_start_year\"]).copy()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"term_start_year\"], errors=\"coerce\")\n",
    "    \n",
    "    # Restrict contract years are 2020-2025\n",
    "    df = df[(df[\"year\"] >= 2020) & (df[\"year\"] <= 2025)].copy()\n",
    "    # Restricts contract terms to 1-5 years\n",
    "    df[\"years_int\"] = pd.to_numeric(df.get(\"years_int\"), errors=\"coerce\")\n",
    "    before = len(df)\n",
    "    df = df[df[\"years_int\"].notna() & (df[\"years_int\"] <= MAX_YEARS)].copy()\n",
    "    print(f\"[FILTER] Dropped {before - len(df)} contracts with years_int > {MAX_YEARS} (or missing)\")\n",
    "\n",
    "    # coerce keys\n",
    "    for c in [\"key_fangraphs\", \"key_mlbam\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Assign pitcher flag\n",
    "    df[\"is_pitcher_flag\"] = df[\"position\"].map(is_pitcher).astype(int) if \"position\" in df.columns else 0\n",
    "\n",
    "    # computes pre_war, post_war, and war_loss\n",
    "    df = add_war_pre_post(\n",
    "        df, war,\n",
    "        contract_key_col=\"key_fangraphs\",\n",
    "        contract_year_col=\"year\",\n",
    "        pre_years=PRE_YEARS,\n",
    "        post_years=POST_YEARS,\n",
    "    )\n",
    "\n",
    "    # full post_war coverage requirements\n",
    "    df[\"war_post_n\"] = pd.to_numeric(df.get(\"war_post_n\"), errors=\"coerce\")\n",
    "    df = df[df[\"war_post_n\"] >= 1].copy()\n",
    "\n",
    "    BAT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"PA\"}\n",
    "    PIT_EXCLUDE = {\"playerId\", \"Season\", \"Name\", \"Tm\", \"IP\", \"TBF\"}\n",
    "\n",
    "    bat_rate_cols = [c for c in bat_rates.columns if c not in BAT_EXCLUDE and c != \"bat_rate_reliability\" and not c.endswith(\"_dup\")]\n",
    "    pit_rate_cols = [c for c in pit_rates.columns if c not in PIT_EXCLUDE and c != \"pit_rate_reliability\" and not c.endswith(\"_dup\")]\n",
    "\n",
    "    # Data integrity\n",
    "    bat_weight_col = \"bat_rate_reliability\" if \"bat_rate_reliability\" in bat_rates.columns else None\n",
    "    pit_weight_col = \"pit_rate_reliability\" if \"pit_rate_reliability\" in pit_rates.columns else None\n",
    "\n",
    "    df = add_pre_rate_features(df, bat_rates, rate_cols=bat_rate_cols, weight_col=bat_weight_col, prefix=\"bat\", pre_years=PRE_YEARS)\n",
    "    df = add_pre_rate_features(df, pit_rates, rate_cols=pit_rate_cols, weight_col=pit_weight_col, prefix=\"pit\", pre_years=PRE_YEARS)\n",
    "\n",
    "    def_feature_cols = [c for c in [\"defensive_runs_saved\", \"fielding_percentage\", \"Errors\"] if c in def_stats.columns]\n",
    "    \n",
    "    df = add_pre_panel_features(\n",
    "        contracts=df,\n",
    "        panel=def_stats,\n",
    "        contract_key_col=\"key_mlbam\",\n",
    "        panel_key_col=\"MLBAMID\",\n",
    "        contract_year_col=\"year\",\n",
    "        panel_year_col=\"year\",\n",
    "        feature_cols=def_feature_cols,\n",
    "        weight_col=\"Innings_played\" if \"Innings_played\" in def_stats.columns else None,\n",
    "        prefix=\"def\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "    \n",
    "    # Add statcast pitching features\n",
    "    sc_feature_cols = [c for c in [\"fastball_avg_speed\",\"whiff_percent\",\"hard_hit_percent\",\"barrel_batted_rate\",\"exit_velocity_avg\",\"swing_percent\"] if c in sc_pit.columns]\n",
    "    \n",
    "    df = add_pre_panel_features(\n",
    "        contracts=df,\n",
    "        panel=sc_pit,\n",
    "        contract_key_col=\"key_mlbam\",\n",
    "        panel_key_col=\"player_id\",\n",
    "        contract_year_col=\"year\",\n",
    "        panel_year_col=\"year\",\n",
    "        feature_cols=sc_feature_cols,\n",
    "        weight_col=\"pa\" if \"pa\" in sc_pit.columns else None,\n",
    "        prefix=\"scpit\",\n",
    "        pre_years=PRE_YEARS,\n",
    "    )\n",
    "\n",
    "    df = dedupe_columns(df)\n",
    "    df_cls = df.copy()\n",
    "\n",
    "    # Data Integrity\n",
    "    # AAV col cleaner\n",
    "    if AAV_COL in df_cls.columns:\n",
    "        df_cls[AAV_COL] = (\n",
    "            df_cls[AAV_COL].astype(str).str.replace(r\"[\\$,]\", \"\", regex=True).str.strip()\n",
    "        )\n",
    "        df_cls[AAV_COL] = pd.to_numeric(df_cls[AAV_COL], errors=\"coerce\")\n",
    "\n",
    "    train_full, test_full = time_split(df_cls, year_col=\"year\")\n",
    "\n",
    "    cut_value = None\n",
    "    if AAV_COL in train_full.columns:\n",
    "        cut_value = float(train_full[AAV_COL].dropna().quantile(REMOVE_TOP_PCTL))\n",
    "        print(f\"[TOP5%] Train-derived AAV cutoff ({REMOVE_TOP_PCTL:.0%}): {cut_value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"[WARN] {AAV_COL} not present; TOP5_REMOVED variant will be skipped.\")\n",
    "\n",
    "    # Detect newly generated features\n",
    "    cov_suffixes = (\"_pre_seasons\", \"_pre_reliability_sum\", \"_pre_weight_sum\")\n",
    "    generated_cov_feats = [\n",
    "        c for c in df_cls.columns\n",
    "        if c in {\"has_bat_pre\",\"has_pit_pre\",\"has_def_pre\",\"has_scpit_pre\"} or c.endswith(cov_suffixes)\n",
    "    ]\n",
    "\n",
    "    PREFIXES = (\"bat_pre_\", \"pit_pre_\", \"def_pre_\", \"scpit_pre_\")\n",
    "    generated_rate_feats = [c for c in df_cls.columns if c.startswith(PREFIXES) and c not in generated_cov_feats]\n",
    "\n",
    "    numeric_features = unique_list([c for c in (BASE_NUMERIC + generated_rate_feats + generated_cov_feats) if c in df_cls.columns])\n",
    "    categorical_features = unique_list([c for c in (BASE_CATEGORICAL) if c in df_cls.columns])\n",
    "\n",
    "    # Runs model parameter sweeps\n",
    "    results_rows = []\n",
    "\n",
    "    def run_one(train_df: pd.DataFrame, test_df: pd.DataFrame, *, variant: str, threshold_value: float, cut_pct: Optional[float], cut_value_used: Optional[float]):\n",
    "        y_train = (pd.to_numeric(train_df[\"war_loss_mean\"], errors=\"coerce\") >= threshold_value).astype(int)\n",
    "        y_test  = (pd.to_numeric(test_df[\"war_loss_mean\"], errors=\"coerce\") >= threshold_value).astype(int)\n",
    "\n",
    "        X_train = train_df[numeric_features + categorical_features]\n",
    "        X_test  = test_df[numeric_features + categorical_features]\n",
    "\n",
    "        for model_name, model in get_classification_models().items():\n",
    "            prob = fit_predict_classification(\n",
    "                model_name, model,\n",
    "                X_train, y_train,\n",
    "                X_test,\n",
    "                numeric_features=numeric_features,\n",
    "                categorical_features=categorical_features,\n",
    "            )\n",
    "\n",
    "            mets = classification_metrics(y_test.values, prob, threshold=0.5)\n",
    "\n",
    "            results_rows.append({\n",
    "                \"run\": \"BASELINE_ONLY\",\n",
    "                \"variant\": variant,\n",
    "                \"war_loss_threshold\": threshold_value,\n",
    "                \"post_years\": POST_YEARS,\n",
    "                \"cut_pct\": cut_pct,\n",
    "                \"cut_value\": cut_value_used,\n",
    "                \"model\": model_name,\n",
    "                \"n_train\": int(len(train_df)),\n",
    "                \"n_test\": int(len(test_df)),\n",
    "                \"pos_rate_train\": float(y_train.mean()) if len(y_train) else np.nan,\n",
    "                \"pos_rate_test\": float(y_test.mean()) if len(y_test) else np.nan,\n",
    "                \"n_features_numeric\": len(numeric_features),\n",
    "                \"n_features_categorical\": len(categorical_features),\n",
    "                **mets,\n",
    "            })\n",
    "\n",
    "    # setup differently than regression due to repeated errors\n",
    "    # assumption is errors were caused by varying model types\n",
    "    for thr in WAR_LOSS_THRESHOLDS:\n",
    "        # UNFILTERED\n",
    "        run_one(train_full, test_full, variant=\"UNFILTERED\", threshold_value=thr, cut_pct=None, cut_value_used=None)\n",
    "\n",
    "        # TOP5_REMOVED\n",
    "        if (cut_value is not None) and (AAV_COL in train_full.columns):\n",
    "            train_cut = train_full[train_full[AAV_COL] <= cut_value].copy()\n",
    "            test_cut  = test_full[test_full[AAV_COL] <= cut_value].copy()\n",
    "            run_one(train_cut, test_cut, variant=\"TOP5_REMOVED\", threshold_value=thr, cut_pct=REMOVE_TOP_PCTL, cut_value_used=cut_value)\n",
    "\n",
    "    results = pd.DataFrame(results_rows)\n",
    "    results = results.sort_values([\"variant\", \"war_loss_threshold\", \"AUC\"], ascending=[True, True, False])\n",
    "    results.to_csv(OUT_CLS_RESULTS, index=False)\n",
    "\n",
    "    print(\"\\nSaved classification sweep:\", OUT_CLS_RESULTS)\n",
    "\n",
    "    # Quick Report\n",
    "    # Returns best results by AUC\n",
    "    summary = (\n",
    "        results.sort_values([\"variant\",\"war_loss_threshold\",\"AUC\"], ascending=[True,True,False])\n",
    "        .groupby([\"variant\",\"war_loss_threshold\"], as_index=False)\n",
    "        .head(1)\n",
    "    )\n",
    "    print(\"\\nBest model per variant/threshold (by AUC):\")\n",
    "    print(summary[[\"variant\",\"war_loss_threshold\",\"model\",\"AUC\",\"Accuracy\",\"F1\",\"n_train\",\"n_test\",\"pos_rate_test\"]].round(4).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabr_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
